{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "from IPython.display import display\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "data = pd.read_csv('../processed_data/v5_obs_combined/all_once_rf_impute.csv' , index_col='src_id')\n",
    "data = data[data['class'].isin(['CV' , 'PL' , 'NS' , 'BH' ,'TC' ])]\n",
    "display(data)\n",
    "data['class'].value_counts()\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "       class  significance  photflux_aper_hilim_u  photflux_aper_hilim_b  \\\n",
       "src_id                                                                     \n",
       "BH0001    BH     -0.092963              -0.321141               0.442908   \n",
       "BH0003    BH     -0.143817              -0.221056               0.454411   \n",
       "BH0004    BH     -0.272880              -1.341423               0.114878   \n",
       "BH0006    BH      0.205001              -0.201856               1.230125   \n",
       "BH0008    BH     -0.400107              -1.325881              -0.741700   \n",
       "...      ...           ...                    ...                    ...   \n",
       "TC0057    TC     -0.090209              -0.895975              -0.095498   \n",
       "TC0058    TC     -0.274165              -0.744381              -0.328065   \n",
       "TC0059    TC     -0.434988              -0.358258              -1.080964   \n",
       "TC0060    TC     -0.123072              -0.099941               0.236588   \n",
       "TC0061    TC      0.175994               0.011715               0.406506   \n",
       "\n",
       "        photflux_aper_lolim_h  photflux_aper_m  photflux_aper_hilim_m  \\\n",
       "src_id                                                                  \n",
       "BH0001               0.515656         0.589796               0.549171   \n",
       "BH0003               0.766986         0.732369               0.705112   \n",
       "BH0004               0.581779         0.308017               0.280342   \n",
       "BH0006               1.314389         1.473151               1.454381   \n",
       "BH0008              -0.022781        -0.760228              -0.735739   \n",
       "...                       ...              ...                    ...   \n",
       "TC0057               0.330975        -0.100632              -0.118408   \n",
       "TC0058              -0.174223        -0.305258              -0.264299   \n",
       "TC0059              -1.564248        -0.829224              -0.747356   \n",
       "TC0060               0.335444         0.267716               0.297755   \n",
       "TC0061               0.358106         0.491677               0.498736   \n",
       "\n",
       "        photflux_aper_hilim_s  photflux_aper_lolim_m  photflux_aper_lolim_b  \\\n",
       "src_id                                                                        \n",
       "BH0001               0.609565               0.695685               0.549074   \n",
       "BH0003               0.069116               0.804934               0.545875   \n",
       "BH0004              -0.635451               0.437881               0.242793   \n",
       "BH0006               1.058235               1.434725               1.221016   \n",
       "BH0008              -1.978598              -0.610354              -0.587607   \n",
       "...                       ...                    ...                    ...   \n",
       "TC0057              -0.749165              -0.006003              -0.074813   \n",
       "TC0058              -0.299743              -0.264944              -0.284775   \n",
       "TC0059              -0.486211              -0.986286              -1.276238   \n",
       "TC0060               0.241717               0.276238               0.270193   \n",
       "TC0061               0.564676               0.538462               0.454505   \n",
       "\n",
       "        ...   bb_ampl  bb_ampl_lolim  bb_kt_hilim  brems_kt_lolim  brems_kt  \\\n",
       "src_id  ...                                                                   \n",
       "BH0001  ... -0.502055      -0.515330    -0.535712       -0.398119 -0.492976   \n",
       "BH0003  ... -0.505944      -0.520658    -0.494571       -0.383140 -0.469388   \n",
       "BH0004  ... -0.505085      -0.519907    -0.486718       -0.427275 -0.507460   \n",
       "BH0006  ... -0.502676      -0.516584    -0.487697       -0.214107 -0.358423   \n",
       "BH0008  ... -0.268649      -0.276070     0.502275       -0.233691 -0.237927   \n",
       "...     ...       ...            ...          ...             ...       ...   \n",
       "TC0057  ... -0.507686      -0.522061    -0.389582        1.239170  3.331563   \n",
       "TC0058  ... -0.280457      -0.278376    -0.215837       -0.253046 -0.246394   \n",
       "TC0059  ...  0.444821       0.291588    -0.055367       -0.234094 -0.253780   \n",
       "TC0060  ... -0.506412      -0.520556    -0.505257       -0.169588 -0.280249   \n",
       "TC0061  ... -0.505061      -0.519153    -0.529618       -0.272366 -0.383922   \n",
       "\n",
       "        brems_kt_hilim  brems_nh_lolim  brems_stat  brems_nh_hilim  brems_nh  \n",
       "src_id                                                                        \n",
       "BH0001       -0.378602       -0.770061   -0.871349       -0.726233 -0.743487  \n",
       "BH0003       -0.376393       -0.586944   -0.072503       -0.545159 -0.557504  \n",
       "BH0004       -0.378873       -0.353273   -1.377949       -0.266964 -0.296700  \n",
       "BH0006       -0.371227       -0.661324   -0.788023       -0.644229 -0.646787  \n",
       "BH0008       -0.282081        0.965514   -0.183550        0.808601  0.806093  \n",
       "...                ...             ...         ...             ...       ...  \n",
       "TC0057        4.411569       -0.598720   -0.749379       -0.547003 -0.567186  \n",
       "TC0058       -0.283647       -0.359334   -0.258631       -0.342759 -0.342818  \n",
       "TC0059       -0.165707        0.075555    0.917218       -0.033115  0.005676  \n",
       "TC0060       -0.360700       -0.757104   -0.622578       -0.717126 -0.732379  \n",
       "TC0061       -0.371107       -0.781478   -0.794665       -0.740620 -0.756692  \n",
       "\n",
       "[317 rows x 91 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>significance</th>\n",
       "      <th>photflux_aper_hilim_u</th>\n",
       "      <th>photflux_aper_hilim_b</th>\n",
       "      <th>photflux_aper_lolim_h</th>\n",
       "      <th>photflux_aper_m</th>\n",
       "      <th>photflux_aper_hilim_m</th>\n",
       "      <th>photflux_aper_hilim_s</th>\n",
       "      <th>photflux_aper_lolim_m</th>\n",
       "      <th>photflux_aper_lolim_b</th>\n",
       "      <th>...</th>\n",
       "      <th>bb_ampl</th>\n",
       "      <th>bb_ampl_lolim</th>\n",
       "      <th>bb_kt_hilim</th>\n",
       "      <th>brems_kt_lolim</th>\n",
       "      <th>brems_kt</th>\n",
       "      <th>brems_kt_hilim</th>\n",
       "      <th>brems_nh_lolim</th>\n",
       "      <th>brems_stat</th>\n",
       "      <th>brems_nh_hilim</th>\n",
       "      <th>brems_nh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BH0001</th>\n",
       "      <td>BH</td>\n",
       "      <td>-0.092963</td>\n",
       "      <td>-0.321141</td>\n",
       "      <td>0.442908</td>\n",
       "      <td>0.515656</td>\n",
       "      <td>0.589796</td>\n",
       "      <td>0.549171</td>\n",
       "      <td>0.609565</td>\n",
       "      <td>0.695685</td>\n",
       "      <td>0.549074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502055</td>\n",
       "      <td>-0.515330</td>\n",
       "      <td>-0.535712</td>\n",
       "      <td>-0.398119</td>\n",
       "      <td>-0.492976</td>\n",
       "      <td>-0.378602</td>\n",
       "      <td>-0.770061</td>\n",
       "      <td>-0.871349</td>\n",
       "      <td>-0.726233</td>\n",
       "      <td>-0.743487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0003</th>\n",
       "      <td>BH</td>\n",
       "      <td>-0.143817</td>\n",
       "      <td>-0.221056</td>\n",
       "      <td>0.454411</td>\n",
       "      <td>0.766986</td>\n",
       "      <td>0.732369</td>\n",
       "      <td>0.705112</td>\n",
       "      <td>0.069116</td>\n",
       "      <td>0.804934</td>\n",
       "      <td>0.545875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505944</td>\n",
       "      <td>-0.520658</td>\n",
       "      <td>-0.494571</td>\n",
       "      <td>-0.383140</td>\n",
       "      <td>-0.469388</td>\n",
       "      <td>-0.376393</td>\n",
       "      <td>-0.586944</td>\n",
       "      <td>-0.072503</td>\n",
       "      <td>-0.545159</td>\n",
       "      <td>-0.557504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0004</th>\n",
       "      <td>BH</td>\n",
       "      <td>-0.272880</td>\n",
       "      <td>-1.341423</td>\n",
       "      <td>0.114878</td>\n",
       "      <td>0.581779</td>\n",
       "      <td>0.308017</td>\n",
       "      <td>0.280342</td>\n",
       "      <td>-0.635451</td>\n",
       "      <td>0.437881</td>\n",
       "      <td>0.242793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505085</td>\n",
       "      <td>-0.519907</td>\n",
       "      <td>-0.486718</td>\n",
       "      <td>-0.427275</td>\n",
       "      <td>-0.507460</td>\n",
       "      <td>-0.378873</td>\n",
       "      <td>-0.353273</td>\n",
       "      <td>-1.377949</td>\n",
       "      <td>-0.266964</td>\n",
       "      <td>-0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0006</th>\n",
       "      <td>BH</td>\n",
       "      <td>0.205001</td>\n",
       "      <td>-0.201856</td>\n",
       "      <td>1.230125</td>\n",
       "      <td>1.314389</td>\n",
       "      <td>1.473151</td>\n",
       "      <td>1.454381</td>\n",
       "      <td>1.058235</td>\n",
       "      <td>1.434725</td>\n",
       "      <td>1.221016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502676</td>\n",
       "      <td>-0.516584</td>\n",
       "      <td>-0.487697</td>\n",
       "      <td>-0.214107</td>\n",
       "      <td>-0.358423</td>\n",
       "      <td>-0.371227</td>\n",
       "      <td>-0.661324</td>\n",
       "      <td>-0.788023</td>\n",
       "      <td>-0.644229</td>\n",
       "      <td>-0.646787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0008</th>\n",
       "      <td>BH</td>\n",
       "      <td>-0.400107</td>\n",
       "      <td>-1.325881</td>\n",
       "      <td>-0.741700</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>-0.760228</td>\n",
       "      <td>-0.735739</td>\n",
       "      <td>-1.978598</td>\n",
       "      <td>-0.610354</td>\n",
       "      <td>-0.587607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268649</td>\n",
       "      <td>-0.276070</td>\n",
       "      <td>0.502275</td>\n",
       "      <td>-0.233691</td>\n",
       "      <td>-0.237927</td>\n",
       "      <td>-0.282081</td>\n",
       "      <td>0.965514</td>\n",
       "      <td>-0.183550</td>\n",
       "      <td>0.808601</td>\n",
       "      <td>0.806093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TC0057</th>\n",
       "      <td>TC</td>\n",
       "      <td>-0.090209</td>\n",
       "      <td>-0.895975</td>\n",
       "      <td>-0.095498</td>\n",
       "      <td>0.330975</td>\n",
       "      <td>-0.100632</td>\n",
       "      <td>-0.118408</td>\n",
       "      <td>-0.749165</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.074813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.507686</td>\n",
       "      <td>-0.522061</td>\n",
       "      <td>-0.389582</td>\n",
       "      <td>1.239170</td>\n",
       "      <td>3.331563</td>\n",
       "      <td>4.411569</td>\n",
       "      <td>-0.598720</td>\n",
       "      <td>-0.749379</td>\n",
       "      <td>-0.547003</td>\n",
       "      <td>-0.567186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TC0058</th>\n",
       "      <td>TC</td>\n",
       "      <td>-0.274165</td>\n",
       "      <td>-0.744381</td>\n",
       "      <td>-0.328065</td>\n",
       "      <td>-0.174223</td>\n",
       "      <td>-0.305258</td>\n",
       "      <td>-0.264299</td>\n",
       "      <td>-0.299743</td>\n",
       "      <td>-0.264944</td>\n",
       "      <td>-0.284775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280457</td>\n",
       "      <td>-0.278376</td>\n",
       "      <td>-0.215837</td>\n",
       "      <td>-0.253046</td>\n",
       "      <td>-0.246394</td>\n",
       "      <td>-0.283647</td>\n",
       "      <td>-0.359334</td>\n",
       "      <td>-0.258631</td>\n",
       "      <td>-0.342759</td>\n",
       "      <td>-0.342818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TC0059</th>\n",
       "      <td>TC</td>\n",
       "      <td>-0.434988</td>\n",
       "      <td>-0.358258</td>\n",
       "      <td>-1.080964</td>\n",
       "      <td>-1.564248</td>\n",
       "      <td>-0.829224</td>\n",
       "      <td>-0.747356</td>\n",
       "      <td>-0.486211</td>\n",
       "      <td>-0.986286</td>\n",
       "      <td>-1.276238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444821</td>\n",
       "      <td>0.291588</td>\n",
       "      <td>-0.055367</td>\n",
       "      <td>-0.234094</td>\n",
       "      <td>-0.253780</td>\n",
       "      <td>-0.165707</td>\n",
       "      <td>0.075555</td>\n",
       "      <td>0.917218</td>\n",
       "      <td>-0.033115</td>\n",
       "      <td>0.005676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TC0060</th>\n",
       "      <td>TC</td>\n",
       "      <td>-0.123072</td>\n",
       "      <td>-0.099941</td>\n",
       "      <td>0.236588</td>\n",
       "      <td>0.335444</td>\n",
       "      <td>0.267716</td>\n",
       "      <td>0.297755</td>\n",
       "      <td>0.241717</td>\n",
       "      <td>0.276238</td>\n",
       "      <td>0.270193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506412</td>\n",
       "      <td>-0.520556</td>\n",
       "      <td>-0.505257</td>\n",
       "      <td>-0.169588</td>\n",
       "      <td>-0.280249</td>\n",
       "      <td>-0.360700</td>\n",
       "      <td>-0.757104</td>\n",
       "      <td>-0.622578</td>\n",
       "      <td>-0.717126</td>\n",
       "      <td>-0.732379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TC0061</th>\n",
       "      <td>TC</td>\n",
       "      <td>0.175994</td>\n",
       "      <td>0.011715</td>\n",
       "      <td>0.406506</td>\n",
       "      <td>0.358106</td>\n",
       "      <td>0.491677</td>\n",
       "      <td>0.498736</td>\n",
       "      <td>0.564676</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.454505</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505061</td>\n",
       "      <td>-0.519153</td>\n",
       "      <td>-0.529618</td>\n",
       "      <td>-0.272366</td>\n",
       "      <td>-0.383922</td>\n",
       "      <td>-0.371107</td>\n",
       "      <td>-0.781478</td>\n",
       "      <td>-0.794665</td>\n",
       "      <td>-0.740620</td>\n",
       "      <td>-0.756692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 91 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PL    118\n",
       "CV     65\n",
       "TC     59\n",
       "NS     48\n",
       "BH     27\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "data = data.replace('NS' , 'XRB')\n",
    "data = data.replace('BH' , 'XRB')\n",
    "#display(data)\n",
    "data_train = data[data['class'].isin(['XRB' , 'CV' , 'PL'])]\n",
    "data_label = data_train['class']\n",
    "display(data_train)\n",
    "display(data_label)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "       class  significance  photflux_aper_hilim_u  photflux_aper_hilim_b  \\\n",
       "src_id                                                                     \n",
       "BH0001   XRB     -0.092963              -0.321141               0.442908   \n",
       "BH0003   XRB     -0.143817              -0.221056               0.454411   \n",
       "BH0004   XRB     -0.272880              -1.341423               0.114878   \n",
       "BH0006   XRB      0.205001              -0.201856               1.230125   \n",
       "BH0008   XRB     -0.400107              -1.325881              -0.741700   \n",
       "...      ...           ...                    ...                    ...   \n",
       "PL0133    PL      0.276601               0.104718               1.223345   \n",
       "PL0134    PL     -0.431133               0.474750               0.320946   \n",
       "PL0135    PL     -0.375689               0.997181               0.350804   \n",
       "PL0136    PL     -0.335851               0.922356               0.414339   \n",
       "PL0138    PL     -0.224229               1.320271               1.054524   \n",
       "\n",
       "        photflux_aper_lolim_h  photflux_aper_m  photflux_aper_hilim_m  \\\n",
       "src_id                                                                  \n",
       "BH0001               0.515656         0.589796               0.549171   \n",
       "BH0003               0.766986         0.732369               0.705112   \n",
       "BH0004               0.581779         0.308017               0.280342   \n",
       "BH0006               1.314389         1.473151               1.454381   \n",
       "BH0008              -0.022781        -0.760228              -0.735739   \n",
       "...                       ...              ...                    ...   \n",
       "PL0133               1.390954         1.232316               1.204622   \n",
       "PL0134              -0.401373         0.495286               0.525492   \n",
       "PL0135              -0.099807         0.499493               0.482792   \n",
       "PL0136               0.086810         0.311214               0.293994   \n",
       "PL0138               1.146765         1.111993               1.095173   \n",
       "\n",
       "        photflux_aper_hilim_s  photflux_aper_lolim_m  photflux_aper_lolim_b  \\\n",
       "src_id                                                                        \n",
       "BH0001               0.609565               0.695685               0.549074   \n",
       "BH0003               0.069116               0.804934               0.545875   \n",
       "BH0004              -0.635451               0.437881               0.242793   \n",
       "BH0006               1.058235               1.434725               1.221016   \n",
       "BH0008              -1.978598              -0.610354              -0.587607   \n",
       "...                       ...                    ...                    ...   \n",
       "PL0133               0.839641               1.235589               1.214471   \n",
       "PL0134               0.878897               0.522618               0.341338   \n",
       "PL0135               0.809518               0.587568               0.430891   \n",
       "PL0136               0.938149               0.423507               0.505374   \n",
       "PL0138               1.126577               1.122899               1.059339   \n",
       "\n",
       "        ...   bb_ampl  bb_ampl_lolim  bb_kt_hilim  brems_kt_lolim  brems_kt  \\\n",
       "src_id  ...                                                                   \n",
       "BH0001  ... -0.502055      -0.515330    -0.535712       -0.398119 -0.492976   \n",
       "BH0003  ... -0.505944      -0.520658    -0.494571       -0.383140 -0.469388   \n",
       "BH0004  ... -0.505085      -0.519907    -0.486718       -0.427275 -0.507460   \n",
       "BH0006  ... -0.502676      -0.516584    -0.487697       -0.214107 -0.358423   \n",
       "BH0008  ... -0.268649      -0.276070     0.502275       -0.233691 -0.237927   \n",
       "...     ...       ...            ...          ...             ...       ...   \n",
       "PL0133  ... -0.506179      -0.520199    -0.425582        3.147032  3.331563   \n",
       "PL0134  ... -0.279692      -0.287526    -0.105759       -0.224856 -0.258647   \n",
       "PL0135  ... -0.281279      -0.289122    -0.083085       -0.214690 -0.236008   \n",
       "PL0136  ... -0.268204      -0.265682    -0.244279       -0.221605 -0.256625   \n",
       "PL0138  ... -0.505151      -0.519119    -0.477958        0.959486  2.106238   \n",
       "\n",
       "        brems_kt_hilim  brems_nh_lolim  brems_stat  brems_nh_hilim  brems_nh  \n",
       "src_id                                                                        \n",
       "BH0001       -0.378602       -0.770061   -0.871349       -0.726233 -0.743487  \n",
       "BH0003       -0.376393       -0.586944   -0.072503       -0.545159 -0.557504  \n",
       "BH0004       -0.378873       -0.353273   -1.377949       -0.266964 -0.296700  \n",
       "BH0006       -0.371227       -0.661324   -0.788023       -0.644229 -0.646787  \n",
       "BH0008       -0.282081        0.965514   -0.183550        0.808601  0.806093  \n",
       "...                ...             ...         ...             ...       ...  \n",
       "PL0133        2.866795       -0.659365   -0.678732       -0.640027 -0.644061  \n",
       "PL0134       -0.294995       -0.451794   -0.405459       -0.414213 -0.429300  \n",
       "PL0135       -0.294977       -0.403847   -0.566087       -0.363035 -0.391829  \n",
       "PL0136       -0.295911       -0.448320    0.011839       -0.414666 -0.429779  \n",
       "PL0138        0.504494       -0.785764   -0.198097       -0.760023 -0.786305  \n",
       "\n",
       "[258 rows x 91 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>significance</th>\n",
       "      <th>photflux_aper_hilim_u</th>\n",
       "      <th>photflux_aper_hilim_b</th>\n",
       "      <th>photflux_aper_lolim_h</th>\n",
       "      <th>photflux_aper_m</th>\n",
       "      <th>photflux_aper_hilim_m</th>\n",
       "      <th>photflux_aper_hilim_s</th>\n",
       "      <th>photflux_aper_lolim_m</th>\n",
       "      <th>photflux_aper_lolim_b</th>\n",
       "      <th>...</th>\n",
       "      <th>bb_ampl</th>\n",
       "      <th>bb_ampl_lolim</th>\n",
       "      <th>bb_kt_hilim</th>\n",
       "      <th>brems_kt_lolim</th>\n",
       "      <th>brems_kt</th>\n",
       "      <th>brems_kt_hilim</th>\n",
       "      <th>brems_nh_lolim</th>\n",
       "      <th>brems_stat</th>\n",
       "      <th>brems_nh_hilim</th>\n",
       "      <th>brems_nh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BH0001</th>\n",
       "      <td>XRB</td>\n",
       "      <td>-0.092963</td>\n",
       "      <td>-0.321141</td>\n",
       "      <td>0.442908</td>\n",
       "      <td>0.515656</td>\n",
       "      <td>0.589796</td>\n",
       "      <td>0.549171</td>\n",
       "      <td>0.609565</td>\n",
       "      <td>0.695685</td>\n",
       "      <td>0.549074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502055</td>\n",
       "      <td>-0.515330</td>\n",
       "      <td>-0.535712</td>\n",
       "      <td>-0.398119</td>\n",
       "      <td>-0.492976</td>\n",
       "      <td>-0.378602</td>\n",
       "      <td>-0.770061</td>\n",
       "      <td>-0.871349</td>\n",
       "      <td>-0.726233</td>\n",
       "      <td>-0.743487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0003</th>\n",
       "      <td>XRB</td>\n",
       "      <td>-0.143817</td>\n",
       "      <td>-0.221056</td>\n",
       "      <td>0.454411</td>\n",
       "      <td>0.766986</td>\n",
       "      <td>0.732369</td>\n",
       "      <td>0.705112</td>\n",
       "      <td>0.069116</td>\n",
       "      <td>0.804934</td>\n",
       "      <td>0.545875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505944</td>\n",
       "      <td>-0.520658</td>\n",
       "      <td>-0.494571</td>\n",
       "      <td>-0.383140</td>\n",
       "      <td>-0.469388</td>\n",
       "      <td>-0.376393</td>\n",
       "      <td>-0.586944</td>\n",
       "      <td>-0.072503</td>\n",
       "      <td>-0.545159</td>\n",
       "      <td>-0.557504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0004</th>\n",
       "      <td>XRB</td>\n",
       "      <td>-0.272880</td>\n",
       "      <td>-1.341423</td>\n",
       "      <td>0.114878</td>\n",
       "      <td>0.581779</td>\n",
       "      <td>0.308017</td>\n",
       "      <td>0.280342</td>\n",
       "      <td>-0.635451</td>\n",
       "      <td>0.437881</td>\n",
       "      <td>0.242793</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505085</td>\n",
       "      <td>-0.519907</td>\n",
       "      <td>-0.486718</td>\n",
       "      <td>-0.427275</td>\n",
       "      <td>-0.507460</td>\n",
       "      <td>-0.378873</td>\n",
       "      <td>-0.353273</td>\n",
       "      <td>-1.377949</td>\n",
       "      <td>-0.266964</td>\n",
       "      <td>-0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0006</th>\n",
       "      <td>XRB</td>\n",
       "      <td>0.205001</td>\n",
       "      <td>-0.201856</td>\n",
       "      <td>1.230125</td>\n",
       "      <td>1.314389</td>\n",
       "      <td>1.473151</td>\n",
       "      <td>1.454381</td>\n",
       "      <td>1.058235</td>\n",
       "      <td>1.434725</td>\n",
       "      <td>1.221016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502676</td>\n",
       "      <td>-0.516584</td>\n",
       "      <td>-0.487697</td>\n",
       "      <td>-0.214107</td>\n",
       "      <td>-0.358423</td>\n",
       "      <td>-0.371227</td>\n",
       "      <td>-0.661324</td>\n",
       "      <td>-0.788023</td>\n",
       "      <td>-0.644229</td>\n",
       "      <td>-0.646787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0008</th>\n",
       "      <td>XRB</td>\n",
       "      <td>-0.400107</td>\n",
       "      <td>-1.325881</td>\n",
       "      <td>-0.741700</td>\n",
       "      <td>-0.022781</td>\n",
       "      <td>-0.760228</td>\n",
       "      <td>-0.735739</td>\n",
       "      <td>-1.978598</td>\n",
       "      <td>-0.610354</td>\n",
       "      <td>-0.587607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268649</td>\n",
       "      <td>-0.276070</td>\n",
       "      <td>0.502275</td>\n",
       "      <td>-0.233691</td>\n",
       "      <td>-0.237927</td>\n",
       "      <td>-0.282081</td>\n",
       "      <td>0.965514</td>\n",
       "      <td>-0.183550</td>\n",
       "      <td>0.808601</td>\n",
       "      <td>0.806093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0133</th>\n",
       "      <td>PL</td>\n",
       "      <td>0.276601</td>\n",
       "      <td>0.104718</td>\n",
       "      <td>1.223345</td>\n",
       "      <td>1.390954</td>\n",
       "      <td>1.232316</td>\n",
       "      <td>1.204622</td>\n",
       "      <td>0.839641</td>\n",
       "      <td>1.235589</td>\n",
       "      <td>1.214471</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506179</td>\n",
       "      <td>-0.520199</td>\n",
       "      <td>-0.425582</td>\n",
       "      <td>3.147032</td>\n",
       "      <td>3.331563</td>\n",
       "      <td>2.866795</td>\n",
       "      <td>-0.659365</td>\n",
       "      <td>-0.678732</td>\n",
       "      <td>-0.640027</td>\n",
       "      <td>-0.644061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0134</th>\n",
       "      <td>PL</td>\n",
       "      <td>-0.431133</td>\n",
       "      <td>0.474750</td>\n",
       "      <td>0.320946</td>\n",
       "      <td>-0.401373</td>\n",
       "      <td>0.495286</td>\n",
       "      <td>0.525492</td>\n",
       "      <td>0.878897</td>\n",
       "      <td>0.522618</td>\n",
       "      <td>0.341338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279692</td>\n",
       "      <td>-0.287526</td>\n",
       "      <td>-0.105759</td>\n",
       "      <td>-0.224856</td>\n",
       "      <td>-0.258647</td>\n",
       "      <td>-0.294995</td>\n",
       "      <td>-0.451794</td>\n",
       "      <td>-0.405459</td>\n",
       "      <td>-0.414213</td>\n",
       "      <td>-0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0135</th>\n",
       "      <td>PL</td>\n",
       "      <td>-0.375689</td>\n",
       "      <td>0.997181</td>\n",
       "      <td>0.350804</td>\n",
       "      <td>-0.099807</td>\n",
       "      <td>0.499493</td>\n",
       "      <td>0.482792</td>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.587568</td>\n",
       "      <td>0.430891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281279</td>\n",
       "      <td>-0.289122</td>\n",
       "      <td>-0.083085</td>\n",
       "      <td>-0.214690</td>\n",
       "      <td>-0.236008</td>\n",
       "      <td>-0.294977</td>\n",
       "      <td>-0.403847</td>\n",
       "      <td>-0.566087</td>\n",
       "      <td>-0.363035</td>\n",
       "      <td>-0.391829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0136</th>\n",
       "      <td>PL</td>\n",
       "      <td>-0.335851</td>\n",
       "      <td>0.922356</td>\n",
       "      <td>0.414339</td>\n",
       "      <td>0.086810</td>\n",
       "      <td>0.311214</td>\n",
       "      <td>0.293994</td>\n",
       "      <td>0.938149</td>\n",
       "      <td>0.423507</td>\n",
       "      <td>0.505374</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268204</td>\n",
       "      <td>-0.265682</td>\n",
       "      <td>-0.244279</td>\n",
       "      <td>-0.221605</td>\n",
       "      <td>-0.256625</td>\n",
       "      <td>-0.295911</td>\n",
       "      <td>-0.448320</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>-0.414666</td>\n",
       "      <td>-0.429779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0138</th>\n",
       "      <td>PL</td>\n",
       "      <td>-0.224229</td>\n",
       "      <td>1.320271</td>\n",
       "      <td>1.054524</td>\n",
       "      <td>1.146765</td>\n",
       "      <td>1.111993</td>\n",
       "      <td>1.095173</td>\n",
       "      <td>1.126577</td>\n",
       "      <td>1.122899</td>\n",
       "      <td>1.059339</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505151</td>\n",
       "      <td>-0.519119</td>\n",
       "      <td>-0.477958</td>\n",
       "      <td>0.959486</td>\n",
       "      <td>2.106238</td>\n",
       "      <td>0.504494</td>\n",
       "      <td>-0.785764</td>\n",
       "      <td>-0.198097</td>\n",
       "      <td>-0.760023</td>\n",
       "      <td>-0.786305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows × 91 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "src_id\n",
       "BH0001    XRB\n",
       "BH0003    XRB\n",
       "BH0004    XRB\n",
       "BH0006    XRB\n",
       "BH0008    XRB\n",
       "         ... \n",
       "PL0133     PL\n",
       "PL0134     PL\n",
       "PL0135     PL\n",
       "PL0136     PL\n",
       "PL0138     PL\n",
       "Name: class, Length: 258, dtype: object"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "\n",
    "x = data_train.copy()\n",
    "y = data_label.copy()\n",
    "\n",
    "x_train_all , x_test_all , y_train , y_test = train_test_split(x , y , test_size=0.2 , shuffle=True, random_state=10 , stratify=y)\n",
    "data_val = data[data['class']=='TC']\n",
    "\n",
    "info_col =  ['class' ,  'significance']\n",
    "\n",
    "\n",
    "x_train = x_train_all.drop(columns = info_col)\n",
    "x_test = x_test_all.drop(columns = info_col)\n",
    "x_val = data_val.drop(columns = info_col)\n",
    "\n",
    "id_train = x_train_all[info_col]\n",
    "id_test = x_test_all[info_col]\n",
    "id_val = data_val[info_col]\n",
    "\n",
    "sig_train = id_train['significance']\n",
    "sig_test = id_test['significance']\n",
    "sig_val = id_val['significance']\n",
    "\n",
    "\n",
    "y_train = y_train.replace('XRB' , 0)\n",
    "y_train = y_train.replace('CV' , 1)\n",
    "y_train = y_train.replace('PL' , 2)\n",
    "print(y_train)\n",
    "display(x_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "src_id\n",
      "NS0021    0\n",
      "PL0020    2\n",
      "NS0054    0\n",
      "PL0051    2\n",
      "BH0028    0\n",
      "         ..\n",
      "NS0041    0\n",
      "PL0055    2\n",
      "CV0088    1\n",
      "NS0075    0\n",
      "PL0118    2\n",
      "Name: class, Length: 206, dtype: int64\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "        photflux_aper_hilim_u  photflux_aper_hilim_b  photflux_aper_lolim_h  \\\n",
       "src_id                                                                        \n",
       "NS0021               1.871105              -0.314038               0.721836   \n",
       "PL0020               1.389882              -1.397577              -1.541882   \n",
       "NS0054              -0.494939              -0.627446              -0.353878   \n",
       "PL0051               0.029461              -0.046562               0.051366   \n",
       "BH0028              -0.302641              -0.317677              -0.229336   \n",
       "...                       ...                    ...                    ...   \n",
       "NS0041              -1.091682              -0.640248               0.060885   \n",
       "PL0055               0.048963               0.037109               0.331987   \n",
       "CV0088              -0.862992              -0.764963              -0.887987   \n",
       "NS0075               0.432656               0.000797              -0.909063   \n",
       "PL0118              -0.313494              -0.166374              -0.718747   \n",
       "\n",
       "        photflux_aper_m  photflux_aper_hilim_m  photflux_aper_hilim_s  \\\n",
       "src_id                                                                  \n",
       "NS0021         0.114779              -0.542571              -0.179024   \n",
       "PL0020        -1.263826              -1.102806              -0.850558   \n",
       "NS0054        -0.705646              -0.659268              -0.500689   \n",
       "PL0051        -0.094078               0.028424               0.144135   \n",
       "BH0028        -0.357387              -0.352802               0.019686   \n",
       "...                 ...                    ...                    ...   \n",
       "NS0041        -0.843651              -0.759745              -1.587375   \n",
       "PL0055         0.083653               0.144754              -0.116210   \n",
       "CV0088        -0.908620              -0.903539              -0.509105   \n",
       "NS0075        -0.174451              -0.134893               0.550435   \n",
       "PL0118        -0.011647               0.039609               0.246368   \n",
       "\n",
       "        photflux_aper_lolim_m  photflux_aper_lolim_b  photflux_aper_s  \\\n",
       "src_id                                                                  \n",
       "NS0021              -0.020510              -0.359482        -0.225870   \n",
       "PL0020              -0.758289              -1.429331        -0.979197   \n",
       "NS0054              -0.607603              -0.506722        -0.501347   \n",
       "PL0051              -0.199681              -0.092303        -0.003043   \n",
       "BH0028              -0.240468              -0.185227         0.055264   \n",
       "...                       ...                    ...              ...   \n",
       "NS0041              -0.822832              -0.542980        -1.685607   \n",
       "PL0055               0.096309               0.047584        -0.268256   \n",
       "CV0088              -1.014333              -0.958380        -0.580193   \n",
       "NS0075              -0.098047               0.086173         0.566666   \n",
       "PL0118               0.027772              -0.122506         0.220474   \n",
       "\n",
       "        photflux_aper_u  ...   bb_ampl  bb_ampl_lolim  bb_kt_hilim  \\\n",
       "src_id                   ...                                         \n",
       "NS0021         1.787914  ... -0.505413      -0.520193    -0.241230   \n",
       "PL0020         1.146362  ...  0.896360       1.130200     0.827855   \n",
       "NS0054        -0.522891  ... -0.279373      -0.281072    -0.059022   \n",
       "PL0051        -0.047147  ... -0.281186      -0.280074    -0.073510   \n",
       "BH0028         0.029976  ... -0.284428      -0.290455    -0.215864   \n",
       "...                 ...  ...       ...            ...          ...   \n",
       "NS0041        -1.018024  ...  0.274339       0.006878     0.496301   \n",
       "PL0055        -0.021678  ... -0.279010      -0.279096    -0.098560   \n",
       "CV0088        -1.358929  ...  0.374900       0.244732    -0.054296   \n",
       "NS0075         0.432165  ...  0.181721       0.099253    -0.065254   \n",
       "PL0118        -0.279089  ...  0.404697       0.527386    -0.073243   \n",
       "\n",
       "        brems_kt_lolim  brems_kt  brems_kt_hilim  brems_nh_lolim  brems_stat  \\\n",
       "src_id                                                                         \n",
       "NS0021        0.828198  1.531214        1.132583        2.367807   -0.665448   \n",
       "PL0020       -0.190689 -0.164128       -0.136373        0.683599    0.412677   \n",
       "NS0054       -0.233408 -0.247607       -0.284123       -0.135062   -0.013263   \n",
       "PL0051       -0.226821 -0.239751       -0.288801       -0.350802   -0.343997   \n",
       "BH0028       -0.240927 -0.259978       -0.305739       -0.319235   -0.531998   \n",
       "...                ...       ...             ...             ...         ...   \n",
       "NS0041       -0.234903 -0.246738       -0.162847        1.044838    0.044880   \n",
       "PL0055       -0.228224 -0.240093       -0.279213       -0.438428   -0.393447   \n",
       "CV0088       -0.235331 -0.250531       -0.169895       -0.067825   -0.013589   \n",
       "NS0075       -0.224067 -0.257762       -0.295051       -0.369002    0.058644   \n",
       "PL0118       -0.244547 -0.260094       -0.288254       -0.454325    0.052008   \n",
       "\n",
       "        brems_nh_hilim  brems_nh  \n",
       "src_id                            \n",
       "NS0021        2.166405  2.392767  \n",
       "PL0020        0.434640  0.516282  \n",
       "NS0054       -0.163514 -0.160539  \n",
       "PL0051       -0.326844 -0.326160  \n",
       "BH0028       -0.260356 -0.311189  \n",
       "...                ...       ...  \n",
       "NS0041        0.936304  0.915807  \n",
       "PL0055       -0.391755 -0.419939  \n",
       "CV0088       -0.130780 -0.092340  \n",
       "NS0075       -0.344971 -0.349055  \n",
       "PL0118       -0.386478 -0.425758  \n",
       "\n",
       "[206 rows x 89 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photflux_aper_hilim_u</th>\n",
       "      <th>photflux_aper_hilim_b</th>\n",
       "      <th>photflux_aper_lolim_h</th>\n",
       "      <th>photflux_aper_m</th>\n",
       "      <th>photflux_aper_hilim_m</th>\n",
       "      <th>photflux_aper_hilim_s</th>\n",
       "      <th>photflux_aper_lolim_m</th>\n",
       "      <th>photflux_aper_lolim_b</th>\n",
       "      <th>photflux_aper_s</th>\n",
       "      <th>photflux_aper_u</th>\n",
       "      <th>...</th>\n",
       "      <th>bb_ampl</th>\n",
       "      <th>bb_ampl_lolim</th>\n",
       "      <th>bb_kt_hilim</th>\n",
       "      <th>brems_kt_lolim</th>\n",
       "      <th>brems_kt</th>\n",
       "      <th>brems_kt_hilim</th>\n",
       "      <th>brems_nh_lolim</th>\n",
       "      <th>brems_stat</th>\n",
       "      <th>brems_nh_hilim</th>\n",
       "      <th>brems_nh</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NS0021</th>\n",
       "      <td>1.871105</td>\n",
       "      <td>-0.314038</td>\n",
       "      <td>0.721836</td>\n",
       "      <td>0.114779</td>\n",
       "      <td>-0.542571</td>\n",
       "      <td>-0.179024</td>\n",
       "      <td>-0.020510</td>\n",
       "      <td>-0.359482</td>\n",
       "      <td>-0.225870</td>\n",
       "      <td>1.787914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505413</td>\n",
       "      <td>-0.520193</td>\n",
       "      <td>-0.241230</td>\n",
       "      <td>0.828198</td>\n",
       "      <td>1.531214</td>\n",
       "      <td>1.132583</td>\n",
       "      <td>2.367807</td>\n",
       "      <td>-0.665448</td>\n",
       "      <td>2.166405</td>\n",
       "      <td>2.392767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0020</th>\n",
       "      <td>1.389882</td>\n",
       "      <td>-1.397577</td>\n",
       "      <td>-1.541882</td>\n",
       "      <td>-1.263826</td>\n",
       "      <td>-1.102806</td>\n",
       "      <td>-0.850558</td>\n",
       "      <td>-0.758289</td>\n",
       "      <td>-1.429331</td>\n",
       "      <td>-0.979197</td>\n",
       "      <td>1.146362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.896360</td>\n",
       "      <td>1.130200</td>\n",
       "      <td>0.827855</td>\n",
       "      <td>-0.190689</td>\n",
       "      <td>-0.164128</td>\n",
       "      <td>-0.136373</td>\n",
       "      <td>0.683599</td>\n",
       "      <td>0.412677</td>\n",
       "      <td>0.434640</td>\n",
       "      <td>0.516282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NS0054</th>\n",
       "      <td>-0.494939</td>\n",
       "      <td>-0.627446</td>\n",
       "      <td>-0.353878</td>\n",
       "      <td>-0.705646</td>\n",
       "      <td>-0.659268</td>\n",
       "      <td>-0.500689</td>\n",
       "      <td>-0.607603</td>\n",
       "      <td>-0.506722</td>\n",
       "      <td>-0.501347</td>\n",
       "      <td>-0.522891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279373</td>\n",
       "      <td>-0.281072</td>\n",
       "      <td>-0.059022</td>\n",
       "      <td>-0.233408</td>\n",
       "      <td>-0.247607</td>\n",
       "      <td>-0.284123</td>\n",
       "      <td>-0.135062</td>\n",
       "      <td>-0.013263</td>\n",
       "      <td>-0.163514</td>\n",
       "      <td>-0.160539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0051</th>\n",
       "      <td>0.029461</td>\n",
       "      <td>-0.046562</td>\n",
       "      <td>0.051366</td>\n",
       "      <td>-0.094078</td>\n",
       "      <td>0.028424</td>\n",
       "      <td>0.144135</td>\n",
       "      <td>-0.199681</td>\n",
       "      <td>-0.092303</td>\n",
       "      <td>-0.003043</td>\n",
       "      <td>-0.047147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281186</td>\n",
       "      <td>-0.280074</td>\n",
       "      <td>-0.073510</td>\n",
       "      <td>-0.226821</td>\n",
       "      <td>-0.239751</td>\n",
       "      <td>-0.288801</td>\n",
       "      <td>-0.350802</td>\n",
       "      <td>-0.343997</td>\n",
       "      <td>-0.326844</td>\n",
       "      <td>-0.326160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BH0028</th>\n",
       "      <td>-0.302641</td>\n",
       "      <td>-0.317677</td>\n",
       "      <td>-0.229336</td>\n",
       "      <td>-0.357387</td>\n",
       "      <td>-0.352802</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>-0.240468</td>\n",
       "      <td>-0.185227</td>\n",
       "      <td>0.055264</td>\n",
       "      <td>0.029976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284428</td>\n",
       "      <td>-0.290455</td>\n",
       "      <td>-0.215864</td>\n",
       "      <td>-0.240927</td>\n",
       "      <td>-0.259978</td>\n",
       "      <td>-0.305739</td>\n",
       "      <td>-0.319235</td>\n",
       "      <td>-0.531998</td>\n",
       "      <td>-0.260356</td>\n",
       "      <td>-0.311189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NS0041</th>\n",
       "      <td>-1.091682</td>\n",
       "      <td>-0.640248</td>\n",
       "      <td>0.060885</td>\n",
       "      <td>-0.843651</td>\n",
       "      <td>-0.759745</td>\n",
       "      <td>-1.587375</td>\n",
       "      <td>-0.822832</td>\n",
       "      <td>-0.542980</td>\n",
       "      <td>-1.685607</td>\n",
       "      <td>-1.018024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274339</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>0.496301</td>\n",
       "      <td>-0.234903</td>\n",
       "      <td>-0.246738</td>\n",
       "      <td>-0.162847</td>\n",
       "      <td>1.044838</td>\n",
       "      <td>0.044880</td>\n",
       "      <td>0.936304</td>\n",
       "      <td>0.915807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0055</th>\n",
       "      <td>0.048963</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.331987</td>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.144754</td>\n",
       "      <td>-0.116210</td>\n",
       "      <td>0.096309</td>\n",
       "      <td>0.047584</td>\n",
       "      <td>-0.268256</td>\n",
       "      <td>-0.021678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279010</td>\n",
       "      <td>-0.279096</td>\n",
       "      <td>-0.098560</td>\n",
       "      <td>-0.228224</td>\n",
       "      <td>-0.240093</td>\n",
       "      <td>-0.279213</td>\n",
       "      <td>-0.438428</td>\n",
       "      <td>-0.393447</td>\n",
       "      <td>-0.391755</td>\n",
       "      <td>-0.419939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV0088</th>\n",
       "      <td>-0.862992</td>\n",
       "      <td>-0.764963</td>\n",
       "      <td>-0.887987</td>\n",
       "      <td>-0.908620</td>\n",
       "      <td>-0.903539</td>\n",
       "      <td>-0.509105</td>\n",
       "      <td>-1.014333</td>\n",
       "      <td>-0.958380</td>\n",
       "      <td>-0.580193</td>\n",
       "      <td>-1.358929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374900</td>\n",
       "      <td>0.244732</td>\n",
       "      <td>-0.054296</td>\n",
       "      <td>-0.235331</td>\n",
       "      <td>-0.250531</td>\n",
       "      <td>-0.169895</td>\n",
       "      <td>-0.067825</td>\n",
       "      <td>-0.013589</td>\n",
       "      <td>-0.130780</td>\n",
       "      <td>-0.092340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NS0075</th>\n",
       "      <td>0.432656</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>-0.909063</td>\n",
       "      <td>-0.174451</td>\n",
       "      <td>-0.134893</td>\n",
       "      <td>0.550435</td>\n",
       "      <td>-0.098047</td>\n",
       "      <td>0.086173</td>\n",
       "      <td>0.566666</td>\n",
       "      <td>0.432165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181721</td>\n",
       "      <td>0.099253</td>\n",
       "      <td>-0.065254</td>\n",
       "      <td>-0.224067</td>\n",
       "      <td>-0.257762</td>\n",
       "      <td>-0.295051</td>\n",
       "      <td>-0.369002</td>\n",
       "      <td>0.058644</td>\n",
       "      <td>-0.344971</td>\n",
       "      <td>-0.349055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PL0118</th>\n",
       "      <td>-0.313494</td>\n",
       "      <td>-0.166374</td>\n",
       "      <td>-0.718747</td>\n",
       "      <td>-0.011647</td>\n",
       "      <td>0.039609</td>\n",
       "      <td>0.246368</td>\n",
       "      <td>0.027772</td>\n",
       "      <td>-0.122506</td>\n",
       "      <td>0.220474</td>\n",
       "      <td>-0.279089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404697</td>\n",
       "      <td>0.527386</td>\n",
       "      <td>-0.073243</td>\n",
       "      <td>-0.244547</td>\n",
       "      <td>-0.260094</td>\n",
       "      <td>-0.288254</td>\n",
       "      <td>-0.454325</td>\n",
       "      <td>0.052008</td>\n",
       "      <td>-0.386478</td>\n",
       "      <td>-0.425758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 89 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "one_hot_y_train =  to_categorical(y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#from tensorflow.keras.layers import Flatten\n",
    "#from tensorflow.keras.layers.convolutional import Conv2D\n",
    "#from tensorflow.keras.layers.pooling import MaxPooling2D\n",
    "visible = layers.Input(shape=(89,1))\n",
    "x = layers.BatchNormalization(axis=-1)(visible)\n",
    "x = layers.Conv1D(32, kernel_size=4, activation='relu')(visible)\n",
    "x = layers.MaxPool1D(pool_size=(2))(x)\n",
    "\n",
    "x = layers.Conv1D(32, kernel_size=4, activation='relu')(x)\n",
    "x = layers.MaxPool1D(pool_size=(2))(x)\n",
    "\n",
    "#x = layers.Conv1D(32, kernel_size=4, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(pool_size=(2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.Dense(10, activation='relu')(x)\n",
    "output = layers.Dense(3, activation='softmax')(x)\n",
    "model_cnn = keras.Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "#print(model_cnn.summary())\n",
    "model_cnn.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics = [\"accuracy\"],\n",
    "    )\n",
    "history = model_cnn.fit(x_train, one_hot_y_train, batch_size=256, epochs=500, validation_split=0.2 , sample_weight=sig_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 988ms/step - loss: 0.0971 - accuracy: 0.3780 - val_loss: -0.1255 - val_accuracy: 0.4048\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0723 - accuracy: 0.4390 - val_loss: -0.1250 - val_accuracy: 0.4048\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0509 - accuracy: 0.4817 - val_loss: -0.1257 - val_accuracy: 0.4524\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0327 - accuracy: 0.4512 - val_loss: -0.1267 - val_accuracy: 0.4762\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0191 - accuracy: 0.4451 - val_loss: -0.1274 - val_accuracy: 0.4524\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0073 - accuracy: 0.4451 - val_loss: -0.1272 - val_accuracy: 0.5000\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -0.0040 - accuracy: 0.4512 - val_loss: -0.1275 - val_accuracy: 0.5000\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -0.0146 - accuracy: 0.4634 - val_loss: -0.1282 - val_accuracy: 0.5000\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -0.0243 - accuracy: 0.4695 - val_loss: -0.1294 - val_accuracy: 0.5000\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.0335 - accuracy: 0.4695 - val_loss: -0.1305 - val_accuracy: 0.5000\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.0420 - accuracy: 0.4634 - val_loss: -0.1314 - val_accuracy: 0.4762\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -0.0500 - accuracy: 0.4695 - val_loss: -0.1317 - val_accuracy: 0.4524\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -0.0577 - accuracy: 0.4634 - val_loss: -0.1321 - val_accuracy: 0.4524\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -0.0649 - accuracy: 0.4573 - val_loss: -0.1329 - val_accuracy: 0.4524\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -0.0720 - accuracy: 0.4329 - val_loss: -0.1337 - val_accuracy: 0.4286\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -0.0790 - accuracy: 0.4390 - val_loss: -0.1347 - val_accuracy: 0.4524\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -0.0861 - accuracy: 0.4390 - val_loss: -0.1362 - val_accuracy: 0.4524\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -0.0935 - accuracy: 0.4268 - val_loss: -0.1380 - val_accuracy: 0.4524\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: -0.1008 - accuracy: 0.4146 - val_loss: -0.1401 - val_accuracy: 0.4524\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -0.1084 - accuracy: 0.4085 - val_loss: -0.1440 - val_accuracy: 0.4524\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -0.1165 - accuracy: 0.3841 - val_loss: -0.1490 - val_accuracy: 0.4524\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.1250 - accuracy: 0.3780 - val_loss: -0.1551 - val_accuracy: 0.4524\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.1341 - accuracy: 0.3720 - val_loss: -0.1625 - val_accuracy: 0.4524\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -0.1439 - accuracy: 0.3598 - val_loss: -0.1709 - val_accuracy: 0.4524\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -0.1544 - accuracy: 0.3598 - val_loss: -0.1803 - val_accuracy: 0.4524\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -0.1653 - accuracy: 0.3598 - val_loss: -0.1909 - val_accuracy: 0.4762\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -0.1773 - accuracy: 0.3537 - val_loss: -0.2028 - val_accuracy: 0.4762\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -0.1907 - accuracy: 0.3537 - val_loss: -0.2156 - val_accuracy: 0.4762\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.2055 - accuracy: 0.3659 - val_loss: -0.2302 - val_accuracy: 0.4524\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -0.2220 - accuracy: 0.3659 - val_loss: -0.2470 - val_accuracy: 0.4524\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.2403 - accuracy: 0.3720 - val_loss: -0.2664 - val_accuracy: 0.4524\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -0.2611 - accuracy: 0.3720 - val_loss: -0.2896 - val_accuracy: 0.4524\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -0.2847 - accuracy: 0.3598 - val_loss: -0.3160 - val_accuracy: 0.4524\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -0.3111 - accuracy: 0.3659 - val_loss: -0.3451 - val_accuracy: 0.4524\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -0.3404 - accuracy: 0.3659 - val_loss: -0.3772 - val_accuracy: 0.4524\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -0.3733 - accuracy: 0.3659 - val_loss: -0.4144 - val_accuracy: 0.4524\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -0.4105 - accuracy: 0.3659 - val_loss: -0.4570 - val_accuracy: 0.4524\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -0.4523 - accuracy: 0.3598 - val_loss: -0.5048 - val_accuracy: 0.4524\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -0.4986 - accuracy: 0.3598 - val_loss: -0.5579 - val_accuracy: 0.4524\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -0.5496 - accuracy: 0.3598 - val_loss: -0.6159 - val_accuracy: 0.4524\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -0.6057 - accuracy: 0.3537 - val_loss: -0.6795 - val_accuracy: 0.4524\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -0.6676 - accuracy: 0.3537 - val_loss: -0.7479 - val_accuracy: 0.4524\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -0.7351 - accuracy: 0.3537 - val_loss: -0.8213 - val_accuracy: 0.4286\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -0.8083 - accuracy: 0.3537 - val_loss: -0.9016 - val_accuracy: 0.4286\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -0.8879 - accuracy: 0.3476 - val_loss: -0.9888 - val_accuracy: 0.4286\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -0.9745 - accuracy: 0.3171 - val_loss: -1.0823 - val_accuracy: 0.4286\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -1.0685 - accuracy: 0.3171 - val_loss: -1.1838 - val_accuracy: 0.4286\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -1.1708 - accuracy: 0.3171 - val_loss: -1.2944 - val_accuracy: 0.4286\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -1.2823 - accuracy: 0.3232 - val_loss: -1.4144 - val_accuracy: 0.4286\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -1.4023 - accuracy: 0.3232 - val_loss: -1.5450 - val_accuracy: 0.4286\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -1.5316 - accuracy: 0.3232 - val_loss: -1.6866 - val_accuracy: 0.4524\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -1.6714 - accuracy: 0.3171 - val_loss: -1.8403 - val_accuracy: 0.4524\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1.8223 - accuracy: 0.3171 - val_loss: -2.0067 - val_accuracy: 0.4524\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: -1.9850 - accuracy: 0.3110 - val_loss: -2.1847 - val_accuracy: 0.4286\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -2.1602 - accuracy: 0.3110 - val_loss: -2.3763 - val_accuracy: 0.4286\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -2.3486 - accuracy: 0.3110 - val_loss: -2.5819 - val_accuracy: 0.4286\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -2.5514 - accuracy: 0.3110 - val_loss: -2.8024 - val_accuracy: 0.4286\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -2.7699 - accuracy: 0.3110 - val_loss: -3.0379 - val_accuracy: 0.4286\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -3.0049 - accuracy: 0.3110 - val_loss: -3.2908 - val_accuracy: 0.4286\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -3.2572 - accuracy: 0.3110 - val_loss: -3.5634 - val_accuracy: 0.4286\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -3.5285 - accuracy: 0.3110 - val_loss: -3.8567 - val_accuracy: 0.4524\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -3.8203 - accuracy: 0.3110 - val_loss: -4.1710 - val_accuracy: 0.4524\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -4.1339 - accuracy: 0.3171 - val_loss: -4.5087 - val_accuracy: 0.4524\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -4.4704 - accuracy: 0.3171 - val_loss: -4.8716 - val_accuracy: 0.4524\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -4.8315 - accuracy: 0.3171 - val_loss: -5.2627 - val_accuracy: 0.4524\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -5.2188 - accuracy: 0.3232 - val_loss: -5.6814 - val_accuracy: 0.4524\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -5.6341 - accuracy: 0.3232 - val_loss: -6.1305 - val_accuracy: 0.4524\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -6.0788 - accuracy: 0.3232 - val_loss: -6.6122 - val_accuracy: 0.4524\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -6.5547 - accuracy: 0.3293 - val_loss: -7.1292 - val_accuracy: 0.4524\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -7.0643 - accuracy: 0.3293 - val_loss: -7.6833 - val_accuracy: 0.4524\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -7.6092 - accuracy: 0.3293 - val_loss: -8.2772 - val_accuracy: 0.4524\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -8.1913 - accuracy: 0.3232 - val_loss: -8.9115 - val_accuracy: 0.4524\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -8.8135 - accuracy: 0.3232 - val_loss: -9.5904 - val_accuracy: 0.4524\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -9.4784 - accuracy: 0.3232 - val_loss: -10.3168 - val_accuracy: 0.4524\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: -10.1880 - accuracy: 0.3232 - val_loss: -11.0891 - val_accuracy: 0.4524\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -10.9442 - accuracy: 0.3232 - val_loss: -11.9125 - val_accuracy: 0.4524\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -11.7492 - accuracy: 0.3232 - val_loss: -12.7900 - val_accuracy: 0.4524\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -12.6070 - accuracy: 0.3232 - val_loss: -13.7223 - val_accuracy: 0.4524\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -13.5197 - accuracy: 0.3232 - val_loss: -14.7100 - val_accuracy: 0.4524\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -14.4912 - accuracy: 0.3232 - val_loss: -15.7568 - val_accuracy: 0.4524\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -15.5252 - accuracy: 0.3293 - val_loss: -16.8619 - val_accuracy: 0.4524\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -16.6257 - accuracy: 0.3293 - val_loss: -18.0361 - val_accuracy: 0.4524\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -17.7972 - accuracy: 0.3293 - val_loss: -19.2939 - val_accuracy: 0.4524\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -19.0408 - accuracy: 0.3293 - val_loss: -20.6291 - val_accuracy: 0.4524\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -20.3641 - accuracy: 0.3232 - val_loss: -22.0515 - val_accuracy: 0.4524\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -21.7748 - accuracy: 0.3232 - val_loss: -23.5513 - val_accuracy: 0.4524\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: -23.2731 - accuracy: 0.3232 - val_loss: -25.1382 - val_accuracy: 0.4524\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -24.8600 - accuracy: 0.3232 - val_loss: -26.8244 - val_accuracy: 0.4524\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -26.5368 - accuracy: 0.3232 - val_loss: -28.6040 - val_accuracy: 0.4524\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -28.3105 - accuracy: 0.3293 - val_loss: -30.4931 - val_accuracy: 0.4524\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -30.1838 - accuracy: 0.3293 - val_loss: -32.4930 - val_accuracy: 0.4524\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -32.1638 - accuracy: 0.3293 - val_loss: -34.6083 - val_accuracy: 0.4524\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -34.2566 - accuracy: 0.3293 - val_loss: -36.8381 - val_accuracy: 0.4524\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -36.4684 - accuracy: 0.3293 - val_loss: -39.1915 - val_accuracy: 0.4524\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -38.8031 - accuracy: 0.3293 - val_loss: -41.6760 - val_accuracy: 0.4524\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: -41.2689 - accuracy: 0.3293 - val_loss: -44.2887 - val_accuracy: 0.4524\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: -43.8693 - accuracy: 0.3293 - val_loss: -47.0343 - val_accuracy: 0.4524\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: -46.6091 - accuracy: 0.3293 - val_loss: -49.9352 - val_accuracy: 0.4524\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -49.4966 - accuracy: 0.3293 - val_loss: -52.9858 - val_accuracy: 0.4524\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -52.5347 - accuracy: 0.3232 - val_loss: -56.1711 - val_accuracy: 0.4524\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -55.7198 - accuracy: 0.3293 - val_loss: -59.5175 - val_accuracy: 0.4524\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -59.0621 - accuracy: 0.3232 - val_loss: -63.0150 - val_accuracy: 0.4524\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -62.5604 - accuracy: 0.3293 - val_loss: -66.6764 - val_accuracy: 0.4524\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -66.2272 - accuracy: 0.3293 - val_loss: -70.5272 - val_accuracy: 0.4524\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -70.0667 - accuracy: 0.3232 - val_loss: -74.5358 - val_accuracy: 0.4524\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -74.0786 - accuracy: 0.3293 - val_loss: -78.7711 - val_accuracy: 0.4524\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -78.2751 - accuracy: 0.3232 - val_loss: -83.1474 - val_accuracy: 0.4524\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -82.6566 - accuracy: 0.3293 - val_loss: -87.7401 - val_accuracy: 0.4524\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: -87.2383 - accuracy: 0.3232 - val_loss: -92.5470 - val_accuracy: 0.4524\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -92.0176 - accuracy: 0.3232 - val_loss: -97.5368 - val_accuracy: 0.4524\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: -97.0064 - accuracy: 0.3293 - val_loss: -102.8135 - val_accuracy: 0.4524\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -102.2202 - accuracy: 0.3232 - val_loss: -108.2893 - val_accuracy: 0.4524\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -107.6585 - accuracy: 0.3232 - val_loss: -113.9936 - val_accuracy: 0.4524\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -113.3398 - accuracy: 0.3232 - val_loss: -119.9582 - val_accuracy: 0.4524\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -119.2754 - accuracy: 0.3232 - val_loss: -126.1600 - val_accuracy: 0.4524\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -125.4682 - accuracy: 0.3232 - val_loss: -132.6424 - val_accuracy: 0.4524\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -131.9279 - accuracy: 0.3232 - val_loss: -139.4464 - val_accuracy: 0.4524\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -138.6649 - accuracy: 0.3232 - val_loss: -146.4969 - val_accuracy: 0.4524\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -145.6852 - accuracy: 0.3232 - val_loss: -153.8538 - val_accuracy: 0.4524\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -153.0033 - accuracy: 0.3232 - val_loss: -161.5151 - val_accuracy: 0.4524\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -160.6315 - accuracy: 0.3232 - val_loss: -169.4564 - val_accuracy: 0.4524\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -168.5770 - accuracy: 0.3232 - val_loss: -177.7597 - val_accuracy: 0.4524\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -176.8510 - accuracy: 0.3232 - val_loss: -186.4139 - val_accuracy: 0.4524\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -185.4646 - accuracy: 0.3232 - val_loss: -195.3934 - val_accuracy: 0.4524\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -194.4314 - accuracy: 0.3232 - val_loss: -204.7131 - val_accuracy: 0.4524\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: -203.7622 - accuracy: 0.3232 - val_loss: -214.3994 - val_accuracy: 0.4524\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -213.4614 - accuracy: 0.3232 - val_loss: -224.5056 - val_accuracy: 0.4524\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -223.5465 - accuracy: 0.3232 - val_loss: -235.0131 - val_accuracy: 0.4524\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -234.0267 - accuracy: 0.3293 - val_loss: -245.9100 - val_accuracy: 0.4524\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -244.9169 - accuracy: 0.3293 - val_loss: -257.2161 - val_accuracy: 0.4524\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -256.2308 - accuracy: 0.3293 - val_loss: -269.0010 - val_accuracy: 0.4524\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -267.9829 - accuracy: 0.3293 - val_loss: -281.2473 - val_accuracy: 0.4524\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -280.1855 - accuracy: 0.3293 - val_loss: -293.9398 - val_accuracy: 0.4524\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -292.8535 - accuracy: 0.3293 - val_loss: -307.0955 - val_accuracy: 0.4524\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -306.0065 - accuracy: 0.3293 - val_loss: -320.7610 - val_accuracy: 0.4524\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -319.6523 - accuracy: 0.3293 - val_loss: -334.9329 - val_accuracy: 0.4524\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -333.8074 - accuracy: 0.3293 - val_loss: -349.6711 - val_accuracy: 0.4524\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -348.4811 - accuracy: 0.3293 - val_loss: -364.8240 - val_accuracy: 0.4524\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -363.6476 - accuracy: 0.3354 - val_loss: -380.6584 - val_accuracy: 0.4524\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -379.1758 - accuracy: 0.3232 - val_loss: -396.4491 - val_accuracy: 0.4524\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -395.0967 - accuracy: 0.3354 - val_loss: -412.9406 - val_accuracy: 0.4524\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -411.5403 - accuracy: 0.3354 - val_loss: -430.1505 - val_accuracy: 0.4524\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -428.6435 - accuracy: 0.3293 - val_loss: -447.8757 - val_accuracy: 0.4524\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -445.6369 - accuracy: 0.3232 - val_loss: -465.6967 - val_accuracy: 0.4524\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -464.1886 - accuracy: 0.3293 - val_loss: -484.0829 - val_accuracy: 0.4524\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -482.7063 - accuracy: 0.3293 - val_loss: -503.1985 - val_accuracy: 0.4524\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -501.8676 - accuracy: 0.3293 - val_loss: -523.2516 - val_accuracy: 0.4524\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -521.7642 - accuracy: 0.3293 - val_loss: -544.0307 - val_accuracy: 0.4524\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -542.3542 - accuracy: 0.3293 - val_loss: -565.5201 - val_accuracy: 0.4524\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -563.5646 - accuracy: 0.3232 - val_loss: -587.0665 - val_accuracy: 0.4524\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -585.3433 - accuracy: 0.3293 - val_loss: -609.3804 - val_accuracy: 0.4524\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -607.7966 - accuracy: 0.3293 - val_loss: -632.5176 - val_accuracy: 0.4524\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -631.0646 - accuracy: 0.3293 - val_loss: -656.5253 - val_accuracy: 0.4524\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -655.0806 - accuracy: 0.3232 - val_loss: -681.6614 - val_accuracy: 0.4524\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -679.9850 - accuracy: 0.3293 - val_loss: -707.6652 - val_accuracy: 0.4524\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: -705.0489 - accuracy: 0.3232 - val_loss: -733.5144 - val_accuracy: 0.4524\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -731.6851 - accuracy: 0.3293 - val_loss: -760.3313 - val_accuracy: 0.4524\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -758.6307 - accuracy: 0.3293 - val_loss: -788.1378 - val_accuracy: 0.4524\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -786.5286 - accuracy: 0.3293 - val_loss: -816.9501 - val_accuracy: 0.4524\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -815.3936 - accuracy: 0.3293 - val_loss: -846.7761 - val_accuracy: 0.4524\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -844.8533 - accuracy: 0.3232 - val_loss: -876.4757 - val_accuracy: 0.4524\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: -875.4117 - accuracy: 0.3232 - val_loss: -907.5236 - val_accuracy: 0.4524\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -906.6301 - accuracy: 0.3293 - val_loss: -939.7347 - val_accuracy: 0.4524\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -938.8968 - accuracy: 0.3293 - val_loss: -973.1174 - val_accuracy: 0.4524\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -972.2548 - accuracy: 0.3293 - val_loss: -1007.7817 - val_accuracy: 0.4524\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1006.5834 - accuracy: 0.3232 - val_loss: -1042.3988 - val_accuracy: 0.4524\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -1041.6191 - accuracy: 0.3293 - val_loss: -1078.3147 - val_accuracy: 0.4524\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -1077.7263 - accuracy: 0.3293 - val_loss: -1115.5499 - val_accuracy: 0.4524\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -1115.0297 - accuracy: 0.3232 - val_loss: -1154.2936 - val_accuracy: 0.4524\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -1153.5476 - accuracy: 0.3293 - val_loss: -1194.3256 - val_accuracy: 0.4524\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1191.8866 - accuracy: 0.3171 - val_loss: -1233.6578 - val_accuracy: 0.4524\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -1233.0354 - accuracy: 0.3293 - val_loss: -1274.4164 - val_accuracy: 0.4524\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -1274.0369 - accuracy: 0.3354 - val_loss: -1317.0212 - val_accuracy: 0.4524\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -1316.8008 - accuracy: 0.3293 - val_loss: -1361.1560 - val_accuracy: 0.4524\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1360.1473 - accuracy: 0.3232 - val_loss: -1405.0748 - val_accuracy: 0.4524\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -1405.0154 - accuracy: 0.3232 - val_loss: -1451.1279 - val_accuracy: 0.4524\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -1451.0775 - accuracy: 0.3293 - val_loss: -1498.7756 - val_accuracy: 0.4524\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -1498.3744 - accuracy: 0.3293 - val_loss: -1548.0232 - val_accuracy: 0.4524\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -1545.9541 - accuracy: 0.3110 - val_loss: -1596.0664 - val_accuracy: 0.4524\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1595.8756 - accuracy: 0.3293 - val_loss: -1645.9541 - val_accuracy: 0.4524\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1645.8501 - accuracy: 0.3354 - val_loss: -1698.1166 - val_accuracy: 0.4524\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -1698.2599 - accuracy: 0.3354 - val_loss: -1752.4783 - val_accuracy: 0.4524\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -1752.3430 - accuracy: 0.3232 - val_loss: -1808.2411 - val_accuracy: 0.4524\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -1804.4800 - accuracy: 0.3110 - val_loss: -1862.6786 - val_accuracy: 0.4524\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -1862.8552 - accuracy: 0.3171 - val_loss: -1916.7920 - val_accuracy: 0.4524\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -1918.0991 - accuracy: 0.3354 - val_loss: -1973.8174 - val_accuracy: 0.4524\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -1975.2107 - accuracy: 0.3354 - val_loss: -2033.6492 - val_accuracy: 0.4524\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -2035.2537 - accuracy: 0.3293 - val_loss: -2096.4075 - val_accuracy: 0.4524\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -2098.0928 - accuracy: 0.3354 - val_loss: -2161.6790 - val_accuracy: 0.4524\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -2162.2427 - accuracy: 0.3232 - val_loss: -2228.6582 - val_accuracy: 0.4524\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -2222.5874 - accuracy: 0.3110 - val_loss: -2294.0352 - val_accuracy: 0.4524\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -2289.6541 - accuracy: 0.3110 - val_loss: -2358.1421 - val_accuracy: 0.4524\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -2358.5769 - accuracy: 0.3293 - val_loss: -2424.7144 - val_accuracy: 0.4524\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -2426.0447 - accuracy: 0.3293 - val_loss: -2493.9558 - val_accuracy: 0.4524\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -2495.4712 - accuracy: 0.3354 - val_loss: -2566.5527 - val_accuracy: 0.4524\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -2568.3301 - accuracy: 0.3293 - val_loss: -2641.8057 - val_accuracy: 0.4524\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -2643.1541 - accuracy: 0.3293 - val_loss: -2719.5991 - val_accuracy: 0.4524\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -2720.0212 - accuracy: 0.3171 - val_loss: -2800.1199 - val_accuracy: 0.4524\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -2793.3147 - accuracy: 0.3110 - val_loss: -2878.6143 - val_accuracy: 0.4524\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -2874.3782 - accuracy: 0.3110 - val_loss: -2955.3503 - val_accuracy: 0.4524\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -2955.4883 - accuracy: 0.3293 - val_loss: -3035.2114 - val_accuracy: 0.4286\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -3035.9556 - accuracy: 0.3354 - val_loss: -3119.2917 - val_accuracy: 0.4286\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -3119.9846 - accuracy: 0.3354 - val_loss: -3207.6067 - val_accuracy: 0.4286\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -3207.7366 - accuracy: 0.3293 - val_loss: -3298.9822 - val_accuracy: 0.4524\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -3297.8110 - accuracy: 0.3293 - val_loss: -3393.2136 - val_accuracy: 0.4524\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -3383.1558 - accuracy: 0.3110 - val_loss: -3485.1492 - val_accuracy: 0.4524\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: -3475.3789 - accuracy: 0.3110 - val_loss: -3575.2573 - val_accuracy: 0.4524\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: -3574.0952 - accuracy: 0.3232 - val_loss: -3668.2905 - val_accuracy: 0.4524\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -3668.5732 - accuracy: 0.3293 - val_loss: -3765.2825 - val_accuracy: 0.4524\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -3766.2012 - accuracy: 0.3293 - val_loss: -3866.1685 - val_accuracy: 0.4524\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -3867.0776 - accuracy: 0.3232 - val_loss: -3971.5229 - val_accuracy: 0.4524\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -3971.1255 - accuracy: 0.3293 - val_loss: -4080.5483 - val_accuracy: 0.4524\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -4075.9265 - accuracy: 0.3171 - val_loss: -4188.0396 - val_accuracy: 0.4524\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -4183.8359 - accuracy: 0.3171 - val_loss: -4294.3047 - val_accuracy: 0.4524\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -4292.8579 - accuracy: 0.3293 - val_loss: -4404.9785 - val_accuracy: 0.4524\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -4403.9727 - accuracy: 0.3293 - val_loss: -4520.0547 - val_accuracy: 0.4524\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -4518.8789 - accuracy: 0.3293 - val_loss: -4639.4023 - val_accuracy: 0.4524\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -4637.4312 - accuracy: 0.3293 - val_loss: -4762.8516 - val_accuracy: 0.4524\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -4755.7095 - accuracy: 0.3171 - val_loss: -4884.4673 - val_accuracy: 0.4524\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -4878.4121 - accuracy: 0.3171 - val_loss: -5004.7476 - val_accuracy: 0.4524\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -5003.7314 - accuracy: 0.3293 - val_loss: -5130.0259 - val_accuracy: 0.4524\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -5130.0430 - accuracy: 0.3232 - val_loss: -5260.6279 - val_accuracy: 0.4524\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: -5260.4746 - accuracy: 0.3171 - val_loss: -5397.3022 - val_accuracy: 0.4524\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -5394.5791 - accuracy: 0.3232 - val_loss: -5537.1724 - val_accuracy: 0.4524\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -5528.3184 - accuracy: 0.3171 - val_loss: -5674.9688 - val_accuracy: 0.4524\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -5668.6260 - accuracy: 0.3171 - val_loss: -5810.2900 - val_accuracy: 0.4286\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -5807.6641 - accuracy: 0.3232 - val_loss: -5952.0015 - val_accuracy: 0.4286\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -5949.5049 - accuracy: 0.3293 - val_loss: -6101.3213 - val_accuracy: 0.4286\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -6098.4126 - accuracy: 0.3232 - val_loss: -6256.4487 - val_accuracy: 0.4286\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -6250.2876 - accuracy: 0.3232 - val_loss: -6415.2061 - val_accuracy: 0.4286\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -6398.0732 - accuracy: 0.3110 - val_loss: -6569.7842 - val_accuracy: 0.4286\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -6556.3535 - accuracy: 0.3171 - val_loss: -6722.3223 - val_accuracy: 0.4286\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -6717.6470 - accuracy: 0.3232 - val_loss: -6879.8354 - val_accuracy: 0.4286\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -6876.7964 - accuracy: 0.3293 - val_loss: -7045.9648 - val_accuracy: 0.4524\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -7042.7349 - accuracy: 0.3232 - val_loss: -7218.0483 - val_accuracy: 0.4524\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -7213.8628 - accuracy: 0.3171 - val_loss: -7394.5698 - val_accuracy: 0.4524\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -7385.7231 - accuracy: 0.3171 - val_loss: -7568.7334 - val_accuracy: 0.4524\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -7562.3530 - accuracy: 0.3171 - val_loss: -7741.1294 - val_accuracy: 0.4524\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -7738.8398 - accuracy: 0.3171 - val_loss: -7922.4761 - val_accuracy: 0.4524\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -7919.5322 - accuracy: 0.3232 - val_loss: -8110.9946 - val_accuracy: 0.4524\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -8106.3027 - accuracy: 0.3232 - val_loss: -8306.2900 - val_accuracy: 0.4524\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: -8297.9668 - accuracy: 0.3171 - val_loss: -8506.4668 - val_accuracy: 0.4524\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -8487.0518 - accuracy: 0.3171 - val_loss: -8702.7266 - val_accuracy: 0.4286\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -8685.1836 - accuracy: 0.3171 - val_loss: -8896.3652 - val_accuracy: 0.4286\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -8887.6182 - accuracy: 0.3171 - val_loss: -9096.4482 - val_accuracy: 0.4286\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -9089.2188 - accuracy: 0.3232 - val_loss: -9305.2852 - val_accuracy: 0.4286\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -9297.4893 - accuracy: 0.3293 - val_loss: -9524.2783 - val_accuracy: 0.4286\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -9512.9980 - accuracy: 0.3171 - val_loss: -9748.6338 - val_accuracy: 0.4524\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -9723.9424 - accuracy: 0.3110 - val_loss: -9967.2764 - val_accuracy: 0.4524\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -9945.7520 - accuracy: 0.3171 - val_loss: -10183.4854 - val_accuracy: 0.4524\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -10172.3350 - accuracy: 0.3171 - val_loss: -10406.7451 - val_accuracy: 0.4524\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -10397.2568 - accuracy: 0.3293 - val_loss: -10641.1895 - val_accuracy: 0.4524\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -10631.2070 - accuracy: 0.3232 - val_loss: -10884.2998 - val_accuracy: 0.4524\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -10870.4600 - accuracy: 0.3171 - val_loss: -11133.6826 - val_accuracy: 0.4286\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -11101.7754 - accuracy: 0.3110 - val_loss: -11377.3770 - val_accuracy: 0.4524\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -11349.5469 - accuracy: 0.3171 - val_loss: -11618.7275 - val_accuracy: 0.4524\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -11604.7334 - accuracy: 0.3110 - val_loss: -11870.0811 - val_accuracy: 0.4524\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -11855.1699 - accuracy: 0.3232 - val_loss: -12129.8555 - val_accuracy: 0.4286\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -12113.0518 - accuracy: 0.3293 - val_loss: -12401.0488 - val_accuracy: 0.4286\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -12379.7451 - accuracy: 0.3171 - val_loss: -12679.3975 - val_accuracy: 0.4048\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -12641.1543 - accuracy: 0.3171 - val_loss: -12953.2451 - val_accuracy: 0.4048\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -12913.4561 - accuracy: 0.3171 - val_loss: -13223.1387 - val_accuracy: 0.4286\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -13197.2139 - accuracy: 0.3171 - val_loss: -13502.1045 - val_accuracy: 0.4286\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -13478.0059 - accuracy: 0.3232 - val_loss: -13792.5137 - val_accuracy: 0.4286\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -13766.2383 - accuracy: 0.3171 - val_loss: -14091.0684 - val_accuracy: 0.4286\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -14058.5117 - accuracy: 0.3110 - val_loss: -14386.1191 - val_accuracy: 0.4286\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: -14357.4150 - accuracy: 0.3171 - val_loss: -14690.1582 - val_accuracy: 0.4286\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -14660.8594 - accuracy: 0.3171 - val_loss: -15003.0000 - val_accuracy: 0.4286\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -14969.3965 - accuracy: 0.3110 - val_loss: -15312.1543 - val_accuracy: 0.4286\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -15283.0850 - accuracy: 0.3232 - val_loss: -15634.1963 - val_accuracy: 0.4286\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -15603.8486 - accuracy: 0.3171 - val_loss: -15965.3389 - val_accuracy: 0.4286\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -15927.6328 - accuracy: 0.3110 - val_loss: -16292.4297 - val_accuracy: 0.4286\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -16260.8125 - accuracy: 0.3171 - val_loss: -16629.5039 - val_accuracy: 0.4286\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -16597.7949 - accuracy: 0.3232 - val_loss: -16978.4453 - val_accuracy: 0.4286\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -16937.2949 - accuracy: 0.3110 - val_loss: -17322.1875 - val_accuracy: 0.4524\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -17289.6367 - accuracy: 0.3171 - val_loss: -17676.0020 - val_accuracy: 0.4524\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -17644.1660 - accuracy: 0.3232 - val_loss: -18044.4473 - val_accuracy: 0.4286\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -17997.7188 - accuracy: 0.3110 - val_loss: -18407.7832 - val_accuracy: 0.4524\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -18367.5605 - accuracy: 0.3110 - val_loss: -18768.5000 - val_accuracy: 0.4524\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -18735.8516 - accuracy: 0.3293 - val_loss: -19148.5957 - val_accuracy: 0.4524\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -19114.2969 - accuracy: 0.3232 - val_loss: -19542.6895 - val_accuracy: 0.4524\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -19504.4082 - accuracy: 0.3232 - val_loss: -19952.2148 - val_accuracy: 0.4286\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -19880.8301 - accuracy: 0.3049 - val_loss: -20350.0586 - val_accuracy: 0.4286\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -20277.6445 - accuracy: 0.3049 - val_loss: -20737.3848 - val_accuracy: 0.4524\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -20693.8594 - accuracy: 0.3171 - val_loss: -21127.7090 - val_accuracy: 0.4286\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: -21086.4883 - accuracy: 0.3293 - val_loss: -21540.0117 - val_accuracy: 0.4286\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -21497.6328 - accuracy: 0.3293 - val_loss: -21972.0781 - val_accuracy: 0.4524\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -21930.0879 - accuracy: 0.3232 - val_loss: -22419.9512 - val_accuracy: 0.4286\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 53ms/step - loss: -22363.6738 - accuracy: 0.3110 - val_loss: -22862.1543 - val_accuracy: 0.4286\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -22794.3457 - accuracy: 0.3110 - val_loss: -23297.3398 - val_accuracy: 0.4286\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -23243.6191 - accuracy: 0.3110 - val_loss: -23728.1641 - val_accuracy: 0.4286\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -23680.3770 - accuracy: 0.3232 - val_loss: -24178.2656 - val_accuracy: 0.4286\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -24131.0195 - accuracy: 0.3232 - val_loss: -24646.3594 - val_accuracy: 0.4286\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -24596.6641 - accuracy: 0.3232 - val_loss: -25131.7109 - val_accuracy: 0.4048\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -25076.4668 - accuracy: 0.3232 - val_loss: -25637.5723 - val_accuracy: 0.4286\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -25545.4004 - accuracy: 0.3049 - val_loss: -26127.8301 - val_accuracy: 0.4286\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -26027.2012 - accuracy: 0.3049 - val_loss: -26604.2227 - val_accuracy: 0.4048\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -26533.2559 - accuracy: 0.3110 - val_loss: -27075.5664 - val_accuracy: 0.4286\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 47ms/step - loss: -27018.1641 - accuracy: 0.3232 - val_loss: -27571.7344 - val_accuracy: 0.4286\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -27511.5664 - accuracy: 0.3293 - val_loss: -28093.3516 - val_accuracy: 0.4286\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -28031.6133 - accuracy: 0.3293 - val_loss: -28640.7285 - val_accuracy: 0.4048\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -28570.6348 - accuracy: 0.3232 - val_loss: -29209.6094 - val_accuracy: 0.4048\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -29083.7656 - accuracy: 0.3049 - val_loss: -29760.3691 - val_accuracy: 0.4286\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -29608.7891 - accuracy: 0.3049 - val_loss: -30296.2852 - val_accuracy: 0.4048\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -30164.6094 - accuracy: 0.3110 - val_loss: -30824.1035 - val_accuracy: 0.4048\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -30734.7402 - accuracy: 0.3171 - val_loss: -31370.8906 - val_accuracy: 0.4048\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: -31288.6562 - accuracy: 0.3232 - val_loss: -31942.7051 - val_accuracy: 0.4048\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -31857.7129 - accuracy: 0.3232 - val_loss: -32538.1934 - val_accuracy: 0.4048\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -32442.3145 - accuracy: 0.3171 - val_loss: -33149.4766 - val_accuracy: 0.4048\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -33021.5586 - accuracy: 0.3110 - val_loss: -33749.1875 - val_accuracy: 0.4048\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -33617.0469 - accuracy: 0.3110 - val_loss: -34339.1523 - val_accuracy: 0.4048\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -34230.7578 - accuracy: 0.3171 - val_loss: -34948.6953 - val_accuracy: 0.4048\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -34842.6055 - accuracy: 0.3171 - val_loss: -35577.1172 - val_accuracy: 0.4048\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -35466.8945 - accuracy: 0.3171 - val_loss: -36223.8203 - val_accuracy: 0.4048\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -36102.4375 - accuracy: 0.3110 - val_loss: -36858.7227 - val_accuracy: 0.4048\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -36742.4727 - accuracy: 0.3171 - val_loss: -37513.7695 - val_accuracy: 0.4048\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: -37394.7148 - accuracy: 0.3232 - val_loss: -38196.0312 - val_accuracy: 0.4048\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -38053.6328 - accuracy: 0.3110 - val_loss: -38865.3477 - val_accuracy: 0.4048\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -38728.1406 - accuracy: 0.3110 - val_loss: -39523.8633 - val_accuracy: 0.4048\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -39400.2422 - accuracy: 0.3232 - val_loss: -40213.8828 - val_accuracy: 0.4048\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 51ms/step - loss: -40088.7148 - accuracy: 0.3232 - val_loss: -40933.5898 - val_accuracy: 0.4048\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -40799.0625 - accuracy: 0.3232 - val_loss: -41680.0703 - val_accuracy: 0.4048\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -41491.7969 - accuracy: 0.3110 - val_loss: -42411.1094 - val_accuracy: 0.4048\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -42205.6719 - accuracy: 0.3110 - val_loss: -43128.7031 - val_accuracy: 0.4048\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -42947.4492 - accuracy: 0.3110 - val_loss: -43834.7617 - val_accuracy: 0.4048\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -43692.3828 - accuracy: 0.3232 - val_loss: -44575.4102 - val_accuracy: 0.4048\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -44434.1055 - accuracy: 0.3293 - val_loss: -45357.2812 - val_accuracy: 0.4048\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -45206.2695 - accuracy: 0.3232 - val_loss: -46168.4883 - val_accuracy: 0.4048\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -45964.7266 - accuracy: 0.3110 - val_loss: -46962.3359 - val_accuracy: 0.4048\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -46739.2383 - accuracy: 0.3110 - val_loss: -47741.1445 - val_accuracy: 0.4048\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -47543.5469 - accuracy: 0.3110 - val_loss: -48507.5352 - val_accuracy: 0.4048\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -48346.6836 - accuracy: 0.3232 - val_loss: -49311.0078 - val_accuracy: 0.4048\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -49152.2852 - accuracy: 0.3232 - val_loss: -50149.7812 - val_accuracy: 0.4048\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -49983.6133 - accuracy: 0.3232 - val_loss: -51021.2031 - val_accuracy: 0.4048\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -50829.5977 - accuracy: 0.3110 - val_loss: -51875.1797 - val_accuracy: 0.4048\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -51675.3398 - accuracy: 0.3110 - val_loss: -52714.3203 - val_accuracy: 0.4048\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -52531.3242 - accuracy: 0.3171 - val_loss: -53580.2031 - val_accuracy: 0.4048\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -53398.7930 - accuracy: 0.3232 - val_loss: -54482.8633 - val_accuracy: 0.4048\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -54287.5000 - accuracy: 0.3171 - val_loss: -55409.4297 - val_accuracy: 0.4048\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -55177.2617 - accuracy: 0.3110 - val_loss: -56316.7930 - val_accuracy: 0.4048\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -56087.7305 - accuracy: 0.3110 - val_loss: -57207.7578 - val_accuracy: 0.4048\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -57006.3555 - accuracy: 0.3171 - val_loss: -58128.3984 - val_accuracy: 0.4048\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -57930.5898 - accuracy: 0.3232 - val_loss: -59089.7969 - val_accuracy: 0.4048\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -58882.3906 - accuracy: 0.3171 - val_loss: -60077.3438 - val_accuracy: 0.4048\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -59846.2930 - accuracy: 0.3110 - val_loss: -61044.7266 - val_accuracy: 0.4048\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -60816.0977 - accuracy: 0.3171 - val_loss: -62040.9219 - val_accuracy: 0.4048\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -61798.6055 - accuracy: 0.3110 - val_loss: -63017.5352 - val_accuracy: 0.4048\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -62791.5352 - accuracy: 0.3171 - val_loss: -64025.3086 - val_accuracy: 0.4048\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -63798.3555 - accuracy: 0.3232 - val_loss: -65076.0586 - val_accuracy: 0.4048\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -64829.9766 - accuracy: 0.3171 - val_loss: -66152.5859 - val_accuracy: 0.4048\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -65842.2500 - accuracy: 0.3110 - val_loss: -67206.1719 - val_accuracy: 0.4048\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -66892.9375 - accuracy: 0.3110 - val_loss: -68240.1016 - val_accuracy: 0.4048\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -67980.1016 - accuracy: 0.3171 - val_loss: -69302.7812 - val_accuracy: 0.4048\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -69050.0156 - accuracy: 0.3232 - val_loss: -70412.5312 - val_accuracy: 0.4048\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -70147.9766 - accuracy: 0.3171 - val_loss: -71551.9141 - val_accuracy: 0.4048\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -71240.0781 - accuracy: 0.3110 - val_loss: -72667.6094 - val_accuracy: 0.4048\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -72366.7734 - accuracy: 0.3110 - val_loss: -73762.5000 - val_accuracy: 0.4048\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -73493.4609 - accuracy: 0.3232 - val_loss: -74909.0078 - val_accuracy: 0.4048\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -74637.1875 - accuracy: 0.3232 - val_loss: -76104.2656 - val_accuracy: 0.4048\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -75815.5156 - accuracy: 0.3232 - val_loss: -77343.7344 - val_accuracy: 0.4048\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -76964.6562 - accuracy: 0.3110 - val_loss: -78554.8516 - val_accuracy: 0.4048\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -78146.6250 - accuracy: 0.3110 - val_loss: -79741.4688 - val_accuracy: 0.4048\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -79371.5859 - accuracy: 0.3110 - val_loss: -80907.1484 - val_accuracy: 0.4048\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -80600.7500 - accuracy: 0.3232 - val_loss: -82129.1016 - val_accuracy: 0.4048\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -81824.5312 - accuracy: 0.3232 - val_loss: -83404.8594 - val_accuracy: 0.4048\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -83086.8516 - accuracy: 0.3232 - val_loss: -84729.5234 - val_accuracy: 0.4048\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -84353.5078 - accuracy: 0.3110 - val_loss: -86024.8125 - val_accuracy: 0.4048\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -85630.5156 - accuracy: 0.3110 - val_loss: -87294.8203 - val_accuracy: 0.4048\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -86944.7422 - accuracy: 0.3171 - val_loss: -88604.0156 - val_accuracy: 0.4048\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -88255.0234 - accuracy: 0.3232 - val_loss: -89967.8125 - val_accuracy: 0.4048\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -89585.0938 - accuracy: 0.3110 - val_loss: -91303.7344 - val_accuracy: 0.4048\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -90934.5234 - accuracy: 0.3171 - val_loss: -92678.7891 - val_accuracy: 0.4048\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -92299.7422 - accuracy: 0.3171 - val_loss: -94091.0234 - val_accuracy: 0.4048\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -93664.5625 - accuracy: 0.3110 - val_loss: -95473.6875 - val_accuracy: 0.4048\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -95075.9453 - accuracy: 0.3110 - val_loss: -96830.5859 - val_accuracy: 0.4048\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -96458.9609 - accuracy: 0.3232 - val_loss: -98252.1797 - val_accuracy: 0.4048\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -97880.0391 - accuracy: 0.3232 - val_loss: -99734.9766 - val_accuracy: 0.4048\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -99344.0547 - accuracy: 0.3232 - val_loss: -101273.2734 - val_accuracy: 0.4048\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -100815.8750 - accuracy: 0.3110 - val_loss: -102776.4297 - val_accuracy: 0.4048\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -102286.6562 - accuracy: 0.3110 - val_loss: -104249.7266 - val_accuracy: 0.4048\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -103808.0391 - accuracy: 0.3110 - val_loss: -105696.4609 - val_accuracy: 0.4048\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -105285.5391 - accuracy: 0.3232 - val_loss: -107212.7891 - val_accuracy: 0.4048\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -106803.8516 - accuracy: 0.3232 - val_loss: -108795.1172 - val_accuracy: 0.4048\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -108368.4062 - accuracy: 0.3232 - val_loss: -110437.5625 - val_accuracy: 0.4048\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -109973.1094 - accuracy: 0.3171 - val_loss: -112115.7734 - val_accuracy: 0.4048\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -111515.5859 - accuracy: 0.3110 - val_loss: -113753.1875 - val_accuracy: 0.4048\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -113102.9766 - accuracy: 0.3049 - val_loss: -115338.0703 - val_accuracy: 0.4048\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -114762.6484 - accuracy: 0.3110 - val_loss: -116895.8906 - val_accuracy: 0.4048\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -116412.6250 - accuracy: 0.3232 - val_loss: -118529.6797 - val_accuracy: 0.4048\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -118051.1797 - accuracy: 0.3232 - val_loss: -120234.7266 - val_accuracy: 0.4048\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -119741.3203 - accuracy: 0.3232 - val_loss: -122005.8828 - val_accuracy: 0.4048\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -121472.1484 - accuracy: 0.3110 - val_loss: -123738.8906 - val_accuracy: 0.4048\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -123185.1484 - accuracy: 0.3110 - val_loss: -125439.0234 - val_accuracy: 0.4048\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -124907.6562 - accuracy: 0.3232 - val_loss: -127213.6172 - val_accuracy: 0.4048\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -126669.0859 - accuracy: 0.3232 - val_loss: -129057.5703 - val_accuracy: 0.4048\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -128441.3203 - accuracy: 0.3110 - val_loss: -130862.3594 - val_accuracy: 0.4048\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -130239.0938 - accuracy: 0.3110 - val_loss: -132633.4688 - val_accuracy: 0.4048\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -132057.5312 - accuracy: 0.3232 - val_loss: -134482.6719 - val_accuracy: 0.4048\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -133895.1406 - accuracy: 0.3171 - val_loss: -136381.8125 - val_accuracy: 0.4048\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -135762.9219 - accuracy: 0.3171 - val_loss: -138328.0625 - val_accuracy: 0.4048\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -137608.0000 - accuracy: 0.3110 - val_loss: -140232.0781 - val_accuracy: 0.4048\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -139513.6719 - accuracy: 0.3110 - val_loss: -142099.3750 - val_accuracy: 0.4048\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -141459.1406 - accuracy: 0.3171 - val_loss: -144024.9688 - val_accuracy: 0.4048\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -143389.6250 - accuracy: 0.3232 - val_loss: -146031.5938 - val_accuracy: 0.4048\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -145368.7812 - accuracy: 0.3171 - val_loss: -148088.4688 - val_accuracy: 0.4048\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -147333.8594 - accuracy: 0.3110 - val_loss: -150100.1094 - val_accuracy: 0.4048\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -149348.0625 - accuracy: 0.3110 - val_loss: -152073.5312 - val_accuracy: 0.4048\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -151387.7188 - accuracy: 0.3232 - val_loss: -154136.2656 - val_accuracy: 0.4048\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -153440.0781 - accuracy: 0.3232 - val_loss: -156282.0938 - val_accuracy: 0.4048\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -155512.3594 - accuracy: 0.3110 - val_loss: -158381.1250 - val_accuracy: 0.4048\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -157615.4531 - accuracy: 0.3110 - val_loss: -160439.9688 - val_accuracy: 0.4048\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -159718.6719 - accuracy: 0.3232 - val_loss: -162591.3281 - val_accuracy: 0.4048\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -161859.3125 - accuracy: 0.3232 - val_loss: -164828.6250 - val_accuracy: 0.4048\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -164050.4688 - accuracy: 0.3110 - val_loss: -167017.7812 - val_accuracy: 0.4048\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -166234.9531 - accuracy: 0.3171 - val_loss: -169267.5312 - val_accuracy: 0.4048\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -168440.3125 - accuracy: 0.3110 - val_loss: -171469.9688 - val_accuracy: 0.4048\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -170675.9062 - accuracy: 0.3171 - val_loss: -173738.0938 - val_accuracy: 0.4048\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -172935.0781 - accuracy: 0.3171 - val_loss: -176069.1406 - val_accuracy: 0.4048\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -175232.4219 - accuracy: 0.3110 - val_loss: -178354.1875 - val_accuracy: 0.4048\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -177528.8281 - accuracy: 0.3232 - val_loss: -180736.8281 - val_accuracy: 0.4048\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -179875.2031 - accuracy: 0.3110 - val_loss: -183069.7812 - val_accuracy: 0.4048\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -182219.1406 - accuracy: 0.3232 - val_loss: -185501.4531 - val_accuracy: 0.4048\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -184615.0625 - accuracy: 0.3171 - val_loss: -187990.6094 - val_accuracy: 0.4048\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -186976.3438 - accuracy: 0.3110 - val_loss: -190425.7500 - val_accuracy: 0.4048\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -189417.1562 - accuracy: 0.3110 - val_loss: -192814.3125 - val_accuracy: 0.4048\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -191906.3750 - accuracy: 0.3171 - val_loss: -195276.6406 - val_accuracy: 0.4048\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -194370.0938 - accuracy: 0.3232 - val_loss: -197840.4219 - val_accuracy: 0.4048\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -196904.0781 - accuracy: 0.3171 - val_loss: -200467.9688 - val_accuracy: 0.4048\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -199407.3594 - accuracy: 0.3110 - val_loss: -203038.4688 - val_accuracy: 0.4048\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -201974.3125 - accuracy: 0.3110 - val_loss: -205559.6875 - val_accuracy: 0.4048\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -204589.9688 - accuracy: 0.3232 - val_loss: -208192.0312 - val_accuracy: 0.4048\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -207207.8125 - accuracy: 0.3232 - val_loss: -210926.2344 - val_accuracy: 0.4048\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: -209817.1250 - accuracy: 0.3110 - val_loss: -213598.8281 - val_accuracy: 0.4048\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 48ms/step - loss: -212491.4688 - accuracy: 0.3110 - val_loss: -216217.7188 - val_accuracy: 0.4048\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 49ms/step - loss: -215199.9062 - accuracy: 0.3232 - val_loss: -218950.2344 - val_accuracy: 0.4048\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -217919.8125 - accuracy: 0.3232 - val_loss: -221788.4219 - val_accuracy: 0.4048\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -220668.3906 - accuracy: 0.3110 - val_loss: -224562.5938 - val_accuracy: 0.4048\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -223455.7812 - accuracy: 0.3110 - val_loss: -227282.0000 - val_accuracy: 0.4048\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -226224.0781 - accuracy: 0.3232 - val_loss: -230120.8281 - val_accuracy: 0.4048\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -229051.4219 - accuracy: 0.3232 - val_loss: -233069.1250 - val_accuracy: 0.4048\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -231950.3438 - accuracy: 0.3171 - val_loss: -236082.3750 - val_accuracy: 0.4048\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -234779.8125 - accuracy: 0.3110 - val_loss: -239022.9062 - val_accuracy: 0.4048\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: -237690.4688 - accuracy: 0.3110 - val_loss: -241900.6250 - val_accuracy: 0.4048\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: -240691.7500 - accuracy: 0.3110 - val_loss: -244726.4688 - val_accuracy: 0.4048\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 44ms/step - loss: -243604.2656 - accuracy: 0.3232 - val_loss: -247685.0000 - val_accuracy: 0.4048\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -246570.1875 - accuracy: 0.3232 - val_loss: -250767.7812 - val_accuracy: 0.4048\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -249624.7812 - accuracy: 0.3232 - val_loss: -253966.1250 - val_accuracy: 0.4048\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -252760.5781 - accuracy: 0.3171 - val_loss: -257230.9688 - val_accuracy: 0.4048\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: -255788.3125 - accuracy: 0.3110 - val_loss: -260417.0312 - val_accuracy: 0.4048\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -258875.6094 - accuracy: 0.3049 - val_loss: -263498.7812 - val_accuracy: 0.4048\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -262086.2500 - accuracy: 0.3110 - val_loss: -266524.4375 - val_accuracy: 0.4048\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -265271.0625 - accuracy: 0.3232 - val_loss: -269692.9062 - val_accuracy: 0.4048\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -268443.3438 - accuracy: 0.3232 - val_loss: -272994.5938 - val_accuracy: 0.4048\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -271711.6250 - accuracy: 0.3232 - val_loss: -276418.5312 - val_accuracy: 0.4048\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -275025.1250 - accuracy: 0.3110 - val_loss: -279765.3438 - val_accuracy: 0.4048\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -278312.7188 - accuracy: 0.3110 - val_loss: -283044.9688 - val_accuracy: 0.4048\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -281664.3750 - accuracy: 0.3171 - val_loss: -286418.9062 - val_accuracy: 0.4048\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -285027.0625 - accuracy: 0.3171 - val_loss: -289882.5312 - val_accuracy: 0.4048\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -288443.2500 - accuracy: 0.3171 - val_loss: -293429.9062 - val_accuracy: 0.4048\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -291820.3750 - accuracy: 0.3110 - val_loss: -296895.4688 - val_accuracy: 0.4048\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -295302.8125 - accuracy: 0.3110 - val_loss: -300291.6875 - val_accuracy: 0.4048\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -298828.9062 - accuracy: 0.3232 - val_loss: -303837.1875 - val_accuracy: 0.4048\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -302357.2812 - accuracy: 0.3232 - val_loss: -307520.6562 - val_accuracy: 0.4048\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -305920.0938 - accuracy: 0.3110 - val_loss: -311122.5625 - val_accuracy: 0.4048\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 34ms/step - loss: -309530.0000 - accuracy: 0.3110 - val_loss: -314653.4375 - val_accuracy: 0.4048\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -313128.2812 - accuracy: 0.3232 - val_loss: -318338.4375 - val_accuracy: 0.4048\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 45ms/step - loss: -316793.0312 - accuracy: 0.3232 - val_loss: -322165.1250 - val_accuracy: 0.4048\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -320533.8438 - accuracy: 0.3110 - val_loss: -325903.7812 - val_accuracy: 0.4048\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -324264.9688 - accuracy: 0.3171 - val_loss: -329737.4688 - val_accuracy: 0.4048\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -328016.4688 - accuracy: 0.3110 - val_loss: -333487.6562 - val_accuracy: 0.4048\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: -331827.7188 - accuracy: 0.3171 - val_loss: -337343.1875 - val_accuracy: 0.4048\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 43ms/step - loss: -335668.3438 - accuracy: 0.3171 - val_loss: -341298.2188 - val_accuracy: 0.4048\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -339548.4062 - accuracy: 0.3110 - val_loss: -345166.8125 - val_accuracy: 0.4048\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -343459.2812 - accuracy: 0.3171 - val_loss: -349142.3438 - val_accuracy: 0.4048\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -347412.6250 - accuracy: 0.3171 - val_loss: -353219.3438 - val_accuracy: 0.4048\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -351385.3750 - accuracy: 0.3110 - val_loss: -357206.9688 - val_accuracy: 0.4048\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -355430.7188 - accuracy: 0.3171 - val_loss: -361303.8125 - val_accuracy: 0.4048\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -359496.8438 - accuracy: 0.3110 - val_loss: -365315.5625 - val_accuracy: 0.4048\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -363534.5000 - accuracy: 0.3232 - val_loss: -369500.5000 - val_accuracy: 0.4048\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -367686.9688 - accuracy: 0.3232 - val_loss: -373843.6250 - val_accuracy: 0.4048\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 46ms/step - loss: -371851.8750 - accuracy: 0.3110 - val_loss: -378084.0625 - val_accuracy: 0.4048\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -376053.7188 - accuracy: 0.3110 - val_loss: -382234.5625 - val_accuracy: 0.4048\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -380337.2500 - accuracy: 0.3232 - val_loss: -386558.3438 - val_accuracy: 0.4048\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -384631.0938 - accuracy: 0.3232 - val_loss: -391042.3438 - val_accuracy: 0.4048\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: -388875.4375 - accuracy: 0.3110 - val_loss: -395421.0000 - val_accuracy: 0.4048\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -393235.6875 - accuracy: 0.3110 - val_loss: -399705.3750 - val_accuracy: 0.4048\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 41ms/step - loss: -397706.8125 - accuracy: 0.3232 - val_loss: -404171.0000 - val_accuracy: 0.4048\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -402144.1250 - accuracy: 0.3171 - val_loss: -408746.5312 - val_accuracy: 0.4048\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -406578.5938 - accuracy: 0.3110 - val_loss: -413217.6250 - val_accuracy: 0.4048\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -411138.1875 - accuracy: 0.3110 - val_loss: -417598.0938 - val_accuracy: 0.4048\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -415572.6875 - accuracy: 0.3232 - val_loss: -422179.3438 - val_accuracy: 0.4048\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -420144.3438 - accuracy: 0.3232 - val_loss: -426948.5312 - val_accuracy: 0.4048\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 38ms/step - loss: -424847.5625 - accuracy: 0.3232 - val_loss: -431883.9062 - val_accuracy: 0.4048\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 39ms/step - loss: -429509.5625 - accuracy: 0.3110 - val_loss: -436695.4688 - val_accuracy: 0.4048\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 36ms/step - loss: -434173.1250 - accuracy: 0.3110 - val_loss: -441399.1562 - val_accuracy: 0.4048\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: -438974.8750 - accuracy: 0.3110 - val_loss: -446009.8125 - val_accuracy: 0.4048\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: -443802.5000 - accuracy: 0.3232 - val_loss: -450825.8438 - val_accuracy: 0.4048\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -448612.0625 - accuracy: 0.3232 - val_loss: -455834.8125 - val_accuracy: 0.4048\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 42ms/step - loss: -453549.8438 - accuracy: 0.3171 - val_loss: -460954.6250 - val_accuracy: 0.4048\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 40ms/step - loss: -458375.1250 - accuracy: 0.3110 - val_loss: -465952.0938 - val_accuracy: 0.4048\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#from tensorflow.keras.layers import Flatten\n",
    "#from tensorflow.keras.layers.convolutional import Conv2D\n",
    "#from tensorflow.keras.layers.pooling import MaxPooling2D\n",
    "visible = layers.Input(shape=(89,))\n",
    "#x = layers.BatchNormalization(axis=-1)(visible)\n",
    "\n",
    "x = layers.Dense(16, activation='relu')(visible)\n",
    "\n",
    "#x = layers.Dense(32, activation='relu')(x)\n",
    "output = layers.Dense(3, activation='softmax')(x)\n",
    "model_cnn = keras.Model(inputs=visible, outputs=output)\n",
    "# summarize layers\n",
    "#print(model_cnn.summary())\n",
    "model_cnn.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics = [\"accuracy\"],\n",
    "    )\n",
    "history = model_cnn.fit(x_train, one_hot_y_train, batch_size=256, epochs=200, validation_split=0.2 , )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 1.1711 - accuracy: 0.3720 - val_loss: 1.0774 - val_accuracy: 0.4524\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1447 - accuracy: 0.3841 - val_loss: 1.0559 - val_accuracy: 0.4524\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1219 - accuracy: 0.3963 - val_loss: 1.0394 - val_accuracy: 0.4524\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1022 - accuracy: 0.4390 - val_loss: 1.0270 - val_accuracy: 0.5476\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0852 - accuracy: 0.4451 - val_loss: 1.0168 - val_accuracy: 0.5476\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0699 - accuracy: 0.4390 - val_loss: 1.0081 - val_accuracy: 0.5238\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0559 - accuracy: 0.4695 - val_loss: 1.0005 - val_accuracy: 0.5476\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0431 - accuracy: 0.5061 - val_loss: 0.9935 - val_accuracy: 0.5238\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0312 - accuracy: 0.5000 - val_loss: 0.9866 - val_accuracy: 0.5476\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0198 - accuracy: 0.5244 - val_loss: 0.9801 - val_accuracy: 0.5476\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0088 - accuracy: 0.5305 - val_loss: 0.9738 - val_accuracy: 0.5476\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9983 - accuracy: 0.5305 - val_loss: 0.9680 - val_accuracy: 0.5476\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9881 - accuracy: 0.5366 - val_loss: 0.9625 - val_accuracy: 0.5476\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9782 - accuracy: 0.5488 - val_loss: 0.9572 - val_accuracy: 0.5476\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9687 - accuracy: 0.5793 - val_loss: 0.9520 - val_accuracy: 0.5476\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9598 - accuracy: 0.5793 - val_loss: 0.9476 - val_accuracy: 0.5476\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9512 - accuracy: 0.5793 - val_loss: 0.9440 - val_accuracy: 0.5714\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9432 - accuracy: 0.5793 - val_loss: 0.9408 - val_accuracy: 0.5714\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9354 - accuracy: 0.5854 - val_loss: 0.9379 - val_accuracy: 0.5476\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9279 - accuracy: 0.5915 - val_loss: 0.9353 - val_accuracy: 0.5476\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9207 - accuracy: 0.5854 - val_loss: 0.9327 - val_accuracy: 0.5476\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9138 - accuracy: 0.5854 - val_loss: 0.9304 - val_accuracy: 0.5476\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9073 - accuracy: 0.5854 - val_loss: 0.9280 - val_accuracy: 0.5476\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9010 - accuracy: 0.5915 - val_loss: 0.9253 - val_accuracy: 0.5476\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8949 - accuracy: 0.6098 - val_loss: 0.9225 - val_accuracy: 0.5476\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8890 - accuracy: 0.6159 - val_loss: 0.9200 - val_accuracy: 0.5476\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8832 - accuracy: 0.6341 - val_loss: 0.9176 - val_accuracy: 0.5952\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8776 - accuracy: 0.6280 - val_loss: 0.9153 - val_accuracy: 0.5476\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8719 - accuracy: 0.6341 - val_loss: 0.9133 - val_accuracy: 0.5476\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8664 - accuracy: 0.6402 - val_loss: 0.9116 - val_accuracy: 0.5476\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8609 - accuracy: 0.6341 - val_loss: 0.9101 - val_accuracy: 0.5476\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8555 - accuracy: 0.6341 - val_loss: 0.9088 - val_accuracy: 0.5476\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8503 - accuracy: 0.6341 - val_loss: 0.9079 - val_accuracy: 0.5476\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8451 - accuracy: 0.6402 - val_loss: 0.9072 - val_accuracy: 0.5476\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8400 - accuracy: 0.6463 - val_loss: 0.9068 - val_accuracy: 0.5476\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8351 - accuracy: 0.6463 - val_loss: 0.9068 - val_accuracy: 0.5476\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8303 - accuracy: 0.6524 - val_loss: 0.9070 - val_accuracy: 0.5476\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8256 - accuracy: 0.6524 - val_loss: 0.9073 - val_accuracy: 0.5476\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8211 - accuracy: 0.6524 - val_loss: 0.9075 - val_accuracy: 0.5476\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8165 - accuracy: 0.6524 - val_loss: 0.9078 - val_accuracy: 0.5476\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8121 - accuracy: 0.6585 - val_loss: 0.9082 - val_accuracy: 0.5476\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8077 - accuracy: 0.6585 - val_loss: 0.9084 - val_accuracy: 0.5476\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8032 - accuracy: 0.6585 - val_loss: 0.9086 - val_accuracy: 0.5476\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7987 - accuracy: 0.6646 - val_loss: 0.9088 - val_accuracy: 0.5476\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7942 - accuracy: 0.6707 - val_loss: 0.9088 - val_accuracy: 0.5476\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7898 - accuracy: 0.6707 - val_loss: 0.9087 - val_accuracy: 0.5476\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7855 - accuracy: 0.6707 - val_loss: 0.9084 - val_accuracy: 0.5238\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7812 - accuracy: 0.6768 - val_loss: 0.9080 - val_accuracy: 0.5238\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7769 - accuracy: 0.6829 - val_loss: 0.9075 - val_accuracy: 0.5238\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7728 - accuracy: 0.6890 - val_loss: 0.9069 - val_accuracy: 0.5238\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7689 - accuracy: 0.7012 - val_loss: 0.9065 - val_accuracy: 0.5238\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7650 - accuracy: 0.7073 - val_loss: 0.9061 - val_accuracy: 0.5238\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7611 - accuracy: 0.7073 - val_loss: 0.9058 - val_accuracy: 0.5000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7572 - accuracy: 0.7073 - val_loss: 0.9056 - val_accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7535 - accuracy: 0.7073 - val_loss: 0.9057 - val_accuracy: 0.5000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7499 - accuracy: 0.7073 - val_loss: 0.9059 - val_accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7462 - accuracy: 0.7134 - val_loss: 0.9063 - val_accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7426 - accuracy: 0.7134 - val_loss: 0.9068 - val_accuracy: 0.5000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7391 - accuracy: 0.7134 - val_loss: 0.9075 - val_accuracy: 0.5000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7357 - accuracy: 0.7134 - val_loss: 0.9083 - val_accuracy: 0.5000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7323 - accuracy: 0.7134 - val_loss: 0.9091 - val_accuracy: 0.5000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7288 - accuracy: 0.7134 - val_loss: 0.9098 - val_accuracy: 0.5238\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7253 - accuracy: 0.7134 - val_loss: 0.9105 - val_accuracy: 0.5238\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7218 - accuracy: 0.7134 - val_loss: 0.9111 - val_accuracy: 0.5238\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7183 - accuracy: 0.7134 - val_loss: 0.9117 - val_accuracy: 0.5238\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7149 - accuracy: 0.7134 - val_loss: 0.9126 - val_accuracy: 0.5238\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7115 - accuracy: 0.7134 - val_loss: 0.9138 - val_accuracy: 0.5238\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7081 - accuracy: 0.7256 - val_loss: 0.9152 - val_accuracy: 0.5238\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7048 - accuracy: 0.7256 - val_loss: 0.9168 - val_accuracy: 0.5238\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7014 - accuracy: 0.7256 - val_loss: 0.9184 - val_accuracy: 0.5238\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6981 - accuracy: 0.7256 - val_loss: 0.9199 - val_accuracy: 0.5238\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6948 - accuracy: 0.7256 - val_loss: 0.9212 - val_accuracy: 0.5238\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6915 - accuracy: 0.7256 - val_loss: 0.9225 - val_accuracy: 0.5238\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6882 - accuracy: 0.7256 - val_loss: 0.9236 - val_accuracy: 0.5238\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6849 - accuracy: 0.7256 - val_loss: 0.9247 - val_accuracy: 0.5238\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6816 - accuracy: 0.7256 - val_loss: 0.9259 - val_accuracy: 0.5238\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6783 - accuracy: 0.7256 - val_loss: 0.9271 - val_accuracy: 0.5238\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6750 - accuracy: 0.7256 - val_loss: 0.9284 - val_accuracy: 0.5238\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6717 - accuracy: 0.7256 - val_loss: 0.9296 - val_accuracy: 0.5238\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6685 - accuracy: 0.7256 - val_loss: 0.9309 - val_accuracy: 0.5238\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6652 - accuracy: 0.7256 - val_loss: 0.9323 - val_accuracy: 0.5238\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6620 - accuracy: 0.7317 - val_loss: 0.9338 - val_accuracy: 0.5238\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6589 - accuracy: 0.7317 - val_loss: 0.9353 - val_accuracy: 0.5238\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6557 - accuracy: 0.7317 - val_loss: 0.9369 - val_accuracy: 0.5238\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6527 - accuracy: 0.7317 - val_loss: 0.9383 - val_accuracy: 0.5238\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6496 - accuracy: 0.7378 - val_loss: 0.9397 - val_accuracy: 0.5238\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6465 - accuracy: 0.7439 - val_loss: 0.9410 - val_accuracy: 0.5000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6435 - accuracy: 0.7500 - val_loss: 0.9422 - val_accuracy: 0.5000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6406 - accuracy: 0.7500 - val_loss: 0.9433 - val_accuracy: 0.5238\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6376 - accuracy: 0.7500 - val_loss: 0.9444 - val_accuracy: 0.5238\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6346 - accuracy: 0.7500 - val_loss: 0.9455 - val_accuracy: 0.5238\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6315 - accuracy: 0.7500 - val_loss: 0.9465 - val_accuracy: 0.5238\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6285 - accuracy: 0.7561 - val_loss: 0.9473 - val_accuracy: 0.5238\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6255 - accuracy: 0.7561 - val_loss: 0.9481 - val_accuracy: 0.5238\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6225 - accuracy: 0.7500 - val_loss: 0.9492 - val_accuracy: 0.5238\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6195 - accuracy: 0.7622 - val_loss: 0.9505 - val_accuracy: 0.5238\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6165 - accuracy: 0.7683 - val_loss: 0.9520 - val_accuracy: 0.5238\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6135 - accuracy: 0.7683 - val_loss: 0.9536 - val_accuracy: 0.5238\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6105 - accuracy: 0.7744 - val_loss: 0.9550 - val_accuracy: 0.5238\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6075 - accuracy: 0.7744 - val_loss: 0.9562 - val_accuracy: 0.5238\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6046 - accuracy: 0.7744 - val_loss: 0.9573 - val_accuracy: 0.5238\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6017 - accuracy: 0.7744 - val_loss: 0.9582 - val_accuracy: 0.5238\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5987 - accuracy: 0.7744 - val_loss: 0.9590 - val_accuracy: 0.5238\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5959 - accuracy: 0.7744 - val_loss: 0.9597 - val_accuracy: 0.5238\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5930 - accuracy: 0.7744 - val_loss: 0.9606 - val_accuracy: 0.5238\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5901 - accuracy: 0.7805 - val_loss: 0.9617 - val_accuracy: 0.5238\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5872 - accuracy: 0.7805 - val_loss: 0.9630 - val_accuracy: 0.5238\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5843 - accuracy: 0.7866 - val_loss: 0.9645 - val_accuracy: 0.5238\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5813 - accuracy: 0.7866 - val_loss: 0.9660 - val_accuracy: 0.5238\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5783 - accuracy: 0.7927 - val_loss: 0.9677 - val_accuracy: 0.5238\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5754 - accuracy: 0.7927 - val_loss: 0.9695 - val_accuracy: 0.5238\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5724 - accuracy: 0.7927 - val_loss: 0.9713 - val_accuracy: 0.5238\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5694 - accuracy: 0.7927 - val_loss: 0.9729 - val_accuracy: 0.5238\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5663 - accuracy: 0.7927 - val_loss: 0.9743 - val_accuracy: 0.5238\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5634 - accuracy: 0.7927 - val_loss: 0.9754 - val_accuracy: 0.5238\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5604 - accuracy: 0.7927 - val_loss: 0.9765 - val_accuracy: 0.5238\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5575 - accuracy: 0.7927 - val_loss: 0.9778 - val_accuracy: 0.5476\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5546 - accuracy: 0.7927 - val_loss: 0.9794 - val_accuracy: 0.5476\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5517 - accuracy: 0.7927 - val_loss: 0.9813 - val_accuracy: 0.5476\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5488 - accuracy: 0.7927 - val_loss: 0.9834 - val_accuracy: 0.5476\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5460 - accuracy: 0.8049 - val_loss: 0.9857 - val_accuracy: 0.5476\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5431 - accuracy: 0.8049 - val_loss: 0.9882 - val_accuracy: 0.5476\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5403 - accuracy: 0.8110 - val_loss: 0.9908 - val_accuracy: 0.5476\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5374 - accuracy: 0.8110 - val_loss: 0.9930 - val_accuracy: 0.5476\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5346 - accuracy: 0.8110 - val_loss: 0.9951 - val_accuracy: 0.5476\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5319 - accuracy: 0.8049 - val_loss: 0.9969 - val_accuracy: 0.5476\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5291 - accuracy: 0.8049 - val_loss: 0.9985 - val_accuracy: 0.5714\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5264 - accuracy: 0.8049 - val_loss: 1.0000 - val_accuracy: 0.5714\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5237 - accuracy: 0.8049 - val_loss: 1.0015 - val_accuracy: 0.5714\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5209 - accuracy: 0.8049 - val_loss: 1.0028 - val_accuracy: 0.5714\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5182 - accuracy: 0.8110 - val_loss: 1.0042 - val_accuracy: 0.5714\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5155 - accuracy: 0.8171 - val_loss: 1.0056 - val_accuracy: 0.5714\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5128 - accuracy: 0.8171 - val_loss: 1.0068 - val_accuracy: 0.5714\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5101 - accuracy: 0.8171 - val_loss: 1.0080 - val_accuracy: 0.5714\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5074 - accuracy: 0.8171 - val_loss: 1.0094 - val_accuracy: 0.5714\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5047 - accuracy: 0.8171 - val_loss: 1.0108 - val_accuracy: 0.5714\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5021 - accuracy: 0.8171 - val_loss: 1.0123 - val_accuracy: 0.5714\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4995 - accuracy: 0.8171 - val_loss: 1.0133 - val_accuracy: 0.5714\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4969 - accuracy: 0.8171 - val_loss: 1.0142 - val_accuracy: 0.5714\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4944 - accuracy: 0.8171 - val_loss: 1.0152 - val_accuracy: 0.5714\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4918 - accuracy: 0.8293 - val_loss: 1.0165 - val_accuracy: 0.5714\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4893 - accuracy: 0.8293 - val_loss: 1.0179 - val_accuracy: 0.5952\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4867 - accuracy: 0.8354 - val_loss: 1.0194 - val_accuracy: 0.5952\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4841 - accuracy: 0.8354 - val_loss: 1.0209 - val_accuracy: 0.5714\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4816 - accuracy: 0.8354 - val_loss: 1.0221 - val_accuracy: 0.5714\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4791 - accuracy: 0.8354 - val_loss: 1.0232 - val_accuracy: 0.5714\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4767 - accuracy: 0.8354 - val_loss: 1.0243 - val_accuracy: 0.5714\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4742 - accuracy: 0.8415 - val_loss: 1.0253 - val_accuracy: 0.5714\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4718 - accuracy: 0.8415 - val_loss: 1.0260 - val_accuracy: 0.5952\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4693 - accuracy: 0.8415 - val_loss: 1.0268 - val_accuracy: 0.5952\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4669 - accuracy: 0.8415 - val_loss: 1.0278 - val_accuracy: 0.5952\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4645 - accuracy: 0.8537 - val_loss: 1.0289 - val_accuracy: 0.5952\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4621 - accuracy: 0.8537 - val_loss: 1.0301 - val_accuracy: 0.5952\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4597 - accuracy: 0.8537 - val_loss: 1.0314 - val_accuracy: 0.5952\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4573 - accuracy: 0.8598 - val_loss: 1.0330 - val_accuracy: 0.5952\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4549 - accuracy: 0.8598 - val_loss: 1.0344 - val_accuracy: 0.5952\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4526 - accuracy: 0.8659 - val_loss: 1.0358 - val_accuracy: 0.5952\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4502 - accuracy: 0.8659 - val_loss: 1.0374 - val_accuracy: 0.5714\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4478 - accuracy: 0.8659 - val_loss: 1.0393 - val_accuracy: 0.5714\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4454 - accuracy: 0.8659 - val_loss: 1.0412 - val_accuracy: 0.5714\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4431 - accuracy: 0.8659 - val_loss: 1.0429 - val_accuracy: 0.5476\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4407 - accuracy: 0.8659 - val_loss: 1.0445 - val_accuracy: 0.5476\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4383 - accuracy: 0.8659 - val_loss: 1.0458 - val_accuracy: 0.5476\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4359 - accuracy: 0.8720 - val_loss: 1.0471 - val_accuracy: 0.5476\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4336 - accuracy: 0.8720 - val_loss: 1.0483 - val_accuracy: 0.5476\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4313 - accuracy: 0.8720 - val_loss: 1.0492 - val_accuracy: 0.5476\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4290 - accuracy: 0.8720 - val_loss: 1.0500 - val_accuracy: 0.5476\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4267 - accuracy: 0.8720 - val_loss: 1.0511 - val_accuracy: 0.5476\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4245 - accuracy: 0.8720 - val_loss: 1.0520 - val_accuracy: 0.5238\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4223 - accuracy: 0.8720 - val_loss: 1.0524 - val_accuracy: 0.5238\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4200 - accuracy: 0.8720 - val_loss: 1.0526 - val_accuracy: 0.5238\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4178 - accuracy: 0.8780 - val_loss: 1.0525 - val_accuracy: 0.5238\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4155 - accuracy: 0.8780 - val_loss: 1.0525 - val_accuracy: 0.5238\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4134 - accuracy: 0.8780 - val_loss: 1.0525 - val_accuracy: 0.5238\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4113 - accuracy: 0.8780 - val_loss: 1.0525 - val_accuracy: 0.5238\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4091 - accuracy: 0.8780 - val_loss: 1.0526 - val_accuracy: 0.5238\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4070 - accuracy: 0.8780 - val_loss: 1.0526 - val_accuracy: 0.5238\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4049 - accuracy: 0.8780 - val_loss: 1.0528 - val_accuracy: 0.5238\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4028 - accuracy: 0.8780 - val_loss: 1.0531 - val_accuracy: 0.5238\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4008 - accuracy: 0.8780 - val_loss: 1.0533 - val_accuracy: 0.5238\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3987 - accuracy: 0.8780 - val_loss: 1.0536 - val_accuracy: 0.5238\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3967 - accuracy: 0.8780 - val_loss: 1.0541 - val_accuracy: 0.5476\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3946 - accuracy: 0.8780 - val_loss: 1.0545 - val_accuracy: 0.5476\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3927 - accuracy: 0.8780 - val_loss: 1.0549 - val_accuracy: 0.5476\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3907 - accuracy: 0.8780 - val_loss: 1.0556 - val_accuracy: 0.5476\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3887 - accuracy: 0.8780 - val_loss: 1.0562 - val_accuracy: 0.5476\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3868 - accuracy: 0.8780 - val_loss: 1.0570 - val_accuracy: 0.5476\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3849 - accuracy: 0.8780 - val_loss: 1.0578 - val_accuracy: 0.5476\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3829 - accuracy: 0.8780 - val_loss: 1.0588 - val_accuracy: 0.5476\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3810 - accuracy: 0.8780 - val_loss: 1.0602 - val_accuracy: 0.5476\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3792 - accuracy: 0.8780 - val_loss: 1.0617 - val_accuracy: 0.5476\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3773 - accuracy: 0.8841 - val_loss: 1.0634 - val_accuracy: 0.5476\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3754 - accuracy: 0.8841 - val_loss: 1.0648 - val_accuracy: 0.5476\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3736 - accuracy: 0.8841 - val_loss: 1.0659 - val_accuracy: 0.5476\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3718 - accuracy: 0.8841 - val_loss: 1.0667 - val_accuracy: 0.5476\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3700 - accuracy: 0.8841 - val_loss: 1.0677 - val_accuracy: 0.5476\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3682 - accuracy: 0.8841 - val_loss: 1.0685 - val_accuracy: 0.5476\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3664 - accuracy: 0.8841 - val_loss: 1.0695 - val_accuracy: 0.5476\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3646 - accuracy: 0.8841 - val_loss: 1.0707 - val_accuracy: 0.5476\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3629 - accuracy: 0.8841 - val_loss: 1.0724 - val_accuracy: 0.5476\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "hist = history.history\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist['accuracy'] , label = 'accuracy' , color='k')\n",
    "plt.plot(hist['val_accuracy'] , label = 'val_accuracy' , color='crimson')\n",
    "plt.xlabel('No of epochs')\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist['loss'] , label = 'loss' , color='k')\n",
    "plt.plot(hist['val_loss'] , label='val_loss' , color='crimson')\n",
    "plt.xlabel(\"No of Epochs\")\n",
    "plt.legend()\n",
    "#plt.savefig('plots/bn_model_level_0_rows.png')\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.179796pt\" version=\"1.1\" viewBox=\"0 0 706.903125 263.179796\" width=\"706.903125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-02T11:29:13.508604</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.179796 \nL 706.903125 263.179796 \nL 706.903125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 225.623546 \nL 334.466761 225.623546 \nL 334.466761 8.183546 \nL 30.103125 8.183546 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc3e4482bfc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.937836\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(40.756586 240.221983)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.698416\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 25 -->\n      <g transform=\"translate(72.335916 240.221983)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"113.458995\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 50 -->\n      <g transform=\"translate(107.096495 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.219575\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 75 -->\n      <g transform=\"translate(141.857075 240.221983)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"182.980155\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 100 -->\n      <g transform=\"translate(173.436405 240.221983)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"217.740735\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 125 -->\n      <g transform=\"translate(208.196985 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.501314\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 150 -->\n      <g transform=\"translate(242.957564 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"287.261894\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 175 -->\n      <g transform=\"translate(277.718144 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"322.022474\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 200 -->\n      <g transform=\"translate(312.478724 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- No of epochs -->\n     <g transform=\"translate(149.654474 253.900108)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"167.773438\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"228.955078\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"264.160156\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"295.947266\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"357.470703\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"420.947266\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"482.128906\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"537.109375\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"600.488281\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3ec9168314\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3ec9168314\" y=\"204.914975\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.4 -->\n      <g transform=\"translate(7.2 208.714194)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3ec9168314\" y=\"166.321727\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 170.120946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3ec9168314\" y=\"127.72848\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.6 -->\n      <g transform=\"translate(7.2 131.527699)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3ec9168314\" y=\"89.135232\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.7 -->\n      <g transform=\"translate(7.2 92.934451)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3ec9168314\" y=\"50.541984\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.8 -->\n      <g transform=\"translate(7.2 54.341203)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m3ec9168314\" y=\"11.948737\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.9 -->\n      <g transform=\"translate(7.2 15.747956)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p411541de62)\" d=\"M 43.937836 215.739909 \nL 46.718682 206.326919 \nL 48.109105 189.854196 \nL 49.499529 187.500951 \nL 50.889952 189.854196 \nL 52.280375 178.087962 \nL 53.670798 163.968483 \nL 55.061221 166.321727 \nL 56.451644 156.908749 \nL 57.842068 154.555505 \nL 59.232491 154.555505 \nL 60.622914 152.202237 \nL 62.013337 147.495748 \nL 63.40376 135.729525 \nL 67.57503 135.729525 \nL 70.355876 131.023036 \nL 71.7463 133.376281 \nL 74.527146 133.376281 \nL 75.917569 131.023036 \nL 77.307992 123.963279 \nL 78.698416 121.610035 \nL 80.088839 114.550301 \nL 81.479262 116.903546 \nL 84.260108 112.197056 \nL 85.650531 114.550301 \nL 88.431378 114.550301 \nL 91.212224 109.843812 \nL 92.602647 109.843812 \nL 93.993071 107.490567 \nL 98.16434 107.490567 \nL 99.554763 105.1373 \nL 102.33561 105.1373 \nL 105.116456 100.430811 \nL 107.897302 100.430811 \nL 112.068572 93.371077 \nL 113.458995 88.664588 \nL 114.849418 86.311343 \nL 120.411111 86.311343 \nL 121.801534 83.958099 \nL 135.705766 83.958099 \nL 137.096189 79.251587 \nL 155.171691 79.251587 \nL 156.562114 76.898342 \nL 160.733384 76.898342 \nL 164.904653 69.838608 \nL 170.466346 69.838608 \nL 171.856769 67.485364 \nL 173.247192 67.485364 \nL 174.637616 69.838608 \nL 176.028039 65.132119 \nL 177.418462 62.778875 \nL 178.808885 62.778875 \nL 180.199308 60.42563 \nL 188.541848 60.42563 \nL 189.932271 58.072386 \nL 191.322694 58.072386 \nL 192.713117 55.719118 \nL 194.10354 55.719118 \nL 195.493963 53.365873 \nL 209.398195 53.365873 \nL 210.788619 48.659384 \nL 212.179042 48.659384 \nL 213.569465 46.30614 \nL 216.350311 46.30614 \nL 217.740735 48.659384 \nL 223.302427 48.659384 \nL 226.083274 43.952895 \nL 237.206659 43.952895 \nL 238.597082 39.246406 \nL 239.987506 39.246406 \nL 241.377929 36.893161 \nL 246.939622 36.893161 \nL 248.330045 34.539917 \nL 252.501314 34.539917 \nL 253.891737 29.833405 \nL 256.672584 29.833405 \nL 258.063007 27.48016 \nL 259.45343 27.48016 \nL 260.843853 25.126916 \nL 269.186393 25.126916 \nL 270.576816 22.773671 \nL 280.309778 22.773671 \nL 281.700201 20.420426 \nL 308.118242 20.420426 \nL 309.508665 18.067182 \nL 320.632051 18.067182 \nL 320.632051 18.067182 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p411541de62)\" d=\"M 43.937836 184.699463 \nL 46.718682 184.699463 \nL 48.109105 147.943992 \nL 49.499529 147.943992 \nL 50.889952 157.132848 \nL 52.280375 147.943992 \nL 53.670798 157.132848 \nL 55.061221 147.943992 \nL 64.794184 147.943992 \nL 66.184607 138.755112 \nL 67.57503 138.755112 \nL 68.965453 147.943992 \nL 78.698416 147.943992 \nL 80.088839 129.566256 \nL 81.479262 147.943992 \nL 106.506879 147.943992 \nL 107.897302 157.132848 \nL 114.849418 157.132848 \nL 116.239842 166.321727 \nL 127.363227 166.321727 \nL 128.75365 157.132848 \nL 162.123807 157.132848 \nL 163.51423 166.321727 \nL 164.904653 166.321727 \nL 166.295076 157.132848 \nL 203.836503 157.132848 \nL 205.226926 147.943992 \nL 217.740735 147.943992 \nL 219.131158 138.755112 \nL 238.597082 138.755112 \nL 239.987506 129.566256 \nL 241.377929 129.566256 \nL 242.768352 138.755112 \nL 248.330045 138.755112 \nL 249.720468 129.566256 \nL 260.843853 129.566256 \nL 262.234277 138.755112 \nL 265.015123 138.755112 \nL 266.405546 147.943992 \nL 276.138509 147.943992 \nL 277.528932 157.132848 \nL 294.21401 157.132848 \nL 295.604433 147.943992 \nL 320.632051 147.943992 \nL 320.632051 147.943992 \n\" style=\"fill:none;stroke:#dc143c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 225.623546 \nL 30.103125 8.183546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 334.466761 225.623546 \nL 334.466761 8.183546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 225.623546 \nL 334.466761 225.623546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 8.183546 \nL 334.466761 8.183546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 37.103125 45.817921 \nL 134.046875 45.817921 \nQ 136.046875 45.817921 136.046875 43.817921 \nL 136.046875 15.183546 \nQ 136.046875 13.183546 134.046875 13.183546 \nL 37.103125 13.183546 \nQ 35.103125 13.183546 35.103125 15.183546 \nL 35.103125 43.817921 \nQ 35.103125 45.817921 37.103125 45.817921 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 39.103125 21.281983 \nL 59.103125 21.281983 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_17\">\n     <!-- accuracy -->\n     <g transform=\"translate(67.103125 24.781983)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 39.103125 35.960108 \nL 59.103125 35.960108 \n\" style=\"fill:none;stroke:#dc143c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_18\">\n     <!-- val_accuracy -->\n     <g transform=\"translate(67.103125 39.460108)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_8\">\n    <path d=\"M 395.339489 225.623546 \nL 699.703125 225.623546 \nL 699.703125 8.183546 \nL 395.339489 8.183546 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"409.174199\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0 -->\n      <g transform=\"translate(405.992949 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"443.934779\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 25 -->\n      <g transform=\"translate(437.572279 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"478.695359\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 50 -->\n      <g transform=\"translate(472.332859 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"513.455939\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 75 -->\n      <g transform=\"translate(507.093439 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"548.216518\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 100 -->\n      <g transform=\"translate(538.672768 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"582.977098\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 125 -->\n      <g transform=\"translate(573.433348 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"617.737678\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 150 -->\n      <g transform=\"translate(608.193928 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"652.498258\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 175 -->\n      <g transform=\"translate(642.954508 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"687.258837\" xlink:href=\"#mc3e4482bfc\" y=\"225.623546\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 200 -->\n      <g transform=\"translate(677.715087 240.221983)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_28\">\n     <!-- No of Epochs -->\n     <g transform=\"translate(514.808026 253.900108)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"135.986328\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"167.773438\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"228.955078\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"264.160156\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"295.947266\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"359.130859\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"422.607422\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"483.789062\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"538.769531\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"602.148438\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"206.657966\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 0.4 -->\n      <g transform=\"translate(372.436364 210.457185)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"182.200623\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 0.5 -->\n      <g transform=\"translate(372.436364 185.999842)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"157.74328\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0.6 -->\n      <g transform=\"translate(372.436364 161.542498)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"133.285936\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 0.7 -->\n      <g transform=\"translate(372.436364 137.085155)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"108.828593\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 0.8 -->\n      <g transform=\"translate(372.436364 112.627811)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"84.371249\"/>\n      </g>\n     </g>\n     <g id=\"text_34\">\n      <!-- 0.9 -->\n      <g transform=\"translate(372.436364 88.170468)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"59.913906\"/>\n      </g>\n     </g>\n     <g id=\"text_35\">\n      <!-- 1.0 -->\n      <g transform=\"translate(372.436364 63.713124)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"35.456562\"/>\n      </g>\n     </g>\n     <g id=\"text_36\">\n      <!-- 1.1 -->\n      <g transform=\"translate(372.436364 39.255781)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.339489\" xlink:href=\"#m3ec9168314\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_37\">\n      <!-- 1.2 -->\n      <g transform=\"translate(372.436364 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p55f9fdaf0d)\" d=\"M 409.174199 18.067182 \nL 410.564623 24.527849 \nL 411.955046 30.10776 \nL 413.345469 34.908085 \nL 414.735892 39.074307 \nL 416.126315 42.830284 \nL 418.907162 49.383899 \nL 421.688008 55.083493 \nL 424.468854 60.328685 \nL 427.249701 65.256185 \nL 430.030547 69.751267 \nL 432.811394 73.816597 \nL 435.59224 77.543054 \nL 438.373086 80.991252 \nL 441.153933 84.129411 \nL 445.325202 88.46889 \nL 450.886895 93.934584 \nL 455.058165 97.792809 \nL 459.229434 101.423359 \nL 464.791127 105.863993 \nL 477.304936 115.475808 \nL 484.257052 120.200751 \nL 491.209168 124.564152 \nL 514.846362 138.582124 \nL 524.579324 144.108914 \nL 537.093133 150.770287 \nL 549.606942 157.338742 \nL 581.586675 173.730943 \nL 594.100484 179.728228 \nL 601.0526 182.949006 \nL 616.347255 189.705448 \nL 639.984449 199.565675 \nL 649.717411 203.380873 \nL 660.840797 207.471161 \nL 669.183336 210.360347 \nL 677.525875 213.102954 \nL 685.868414 215.739909 \nL 685.868414 215.739909 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\">\n    <path clip-path=\"url(#p55f9fdaf0d)\" d=\"M 409.174199 40.98253 \nL 410.564623 46.239428 \nL 411.955046 50.271565 \nL 413.345469 53.312184 \nL 414.735892 55.801358 \nL 416.126315 57.93743 \nL 418.907162 61.494523 \nL 421.688008 64.784888 \nL 424.468854 67.748814 \nL 428.640124 71.663105 \nL 430.030547 72.740587 \nL 432.811394 74.385566 \nL 435.59224 75.741293 \nL 445.325202 80.073002 \nL 448.106049 81.129463 \nL 450.886895 81.904735 \nL 453.667741 82.438396 \nL 456.448588 82.696276 \nL 459.229434 82.651624 \nL 470.35282 82.214264 \nL 474.524089 82.409474 \nL 480.085782 82.877739 \nL 482.866628 82.990585 \nL 485.647475 82.930831 \nL 489.818744 82.535673 \nL 495.380437 81.81482 \nL 499.551707 81.291247 \nL 502.332553 80.64716 \nL 507.894246 79.176575 \nL 521.798478 76.110124 \nL 527.360171 74.662105 \nL 532.921863 73.505815 \nL 535.70271 73.000901 \nL 539.873979 72.332892 \nL 544.045249 71.263573 \nL 546.826095 70.62507 \nL 550.997365 69.945777 \nL 553.778211 69.546479 \nL 556.559058 68.955863 \nL 560.730327 67.824079 \nL 564.901597 66.530584 \nL 567.682443 65.931615 \nL 570.463289 65.338287 \nL 573.244136 64.494252 \nL 576.024982 63.40209 \nL 580.196252 61.622895 \nL 582.977098 60.67839 \nL 587.148368 59.555994 \nL 599.662176 56.66602 \nL 602.443023 56.1893 \nL 606.614292 55.157577 \nL 609.395139 54.514263 \nL 613.566408 53.728961 \nL 617.737678 53.110604 \nL 621.908948 52.224308 \nL 630.251487 49.84919 \nL 634.422756 48.705277 \nL 637.203603 48.106891 \nL 644.155719 47.087414 \nL 648.326988 47.070941 \nL 655.279104 47.005166 \nL 663.621643 46.475966 \nL 669.183336 45.783554 \nL 671.964182 45.188754 \nL 677.525875 43.802705 \nL 683.087568 42.90498 \nL 685.868414 42.215455 \nL 685.868414 42.215455 \n\" style=\"fill:none;stroke:#dc143c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 395.339489 225.623546 \nL 395.339489 8.183546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 699.703125 225.623546 \nL 699.703125 8.183546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 395.339489 225.623546 \nL 699.703125 225.623546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path d=\"M 395.339489 8.183546 \nL 699.703125 8.183546 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_2\">\n    <g id=\"patch_13\">\n     <path d=\"M 402.339489 220.623546 \nL 473.480114 220.623546 \nQ 475.480114 220.623546 475.480114 218.623546 \nL 475.480114 189.989171 \nQ 475.480114 187.989171 473.480114 187.989171 \nL 402.339489 187.989171 \nQ 400.339489 187.989171 400.339489 189.989171 \nL 400.339489 218.623546 \nQ 400.339489 220.623546 402.339489 220.623546 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_42\">\n     <path d=\"M 404.339489 196.087608 \nL 424.339489 196.087608 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_43\"/>\n    <g id=\"text_38\">\n     <!-- loss -->\n     <g transform=\"translate(432.339489 199.587608)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n    <g id=\"line2d_44\">\n     <path d=\"M 404.339489 210.765733 \nL 424.339489 210.765733 \n\" style=\"fill:none;stroke:#dc143c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_45\"/>\n    <g id=\"text_39\">\n     <!-- val_loss -->\n     <g transform=\"translate(432.339489 214.265733)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p411541de62\">\n   <rect height=\"217.44\" width=\"304.363636\" x=\"30.103125\" y=\"8.183546\"/>\n  </clipPath>\n  <clipPath id=\"p55f9fdaf0d\">\n   <rect height=\"217.44\" width=\"304.363636\" x=\"395.339489\" y=\"8.183546\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "258*0.2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "51.6"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}